{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "id": "1r0abI6B9Duw"
      },
      "id": "1r0abI6B9Duw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nevergrad"
      ],
      "metadata": {
        "id": "nGZU0I91S1Un"
      },
      "id": "nGZU0I91S1Un",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "metadata": {
        "id": "PXaM31ND4fes"
      },
      "id": "PXaM31ND4fes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nevergrad as ng"
      ],
      "metadata": {
        "id": "K9jgpOtm-n_7"
      },
      "id": "K9jgpOtm-n_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3f19c843-d866-4d4e-af7e-7d38438cdc06",
      "metadata": {
        "id": "3f19c843-d866-4d4e-af7e-7d38438cdc06"
      },
      "source": [
        "# creating mountain-car class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6030d0-44ee-494d-a539-80780b499f83",
      "metadata": {
        "id": "3e6030d0-44ee-494d-a539-80780b499f83"
      },
      "source": [
        "The specifications to note about representing the mountain-car MDP:\n",
        "\n",
        "- the state is 2-dimensional and consists of x-axis position of the car as well as its velocity. Limits are $[-1.2, 0.6]$ for the position and $[-0.07, 0.07]$ for the velocity.\n",
        "- the action is 1-dimensional and represents the force applied to the car. Limits are $[-1, 1]$.\n",
        "- the reward is $100$ if the car reached the goal, i.e. its position is $>=0.45$ along the x-axis. For every timestep, a penalty of $0.1 * \\mathrm{action}^2$ is also applied\n",
        "- the car starts in the valley between the mountains at a location drawn uniformly at random between $-0.6$ & $-0.4$ and\n",
        "$\\gamma = 1$.\n",
        "- the dynamics are given by the following equations:\n",
        "$$v_{t+1} = v_t + a_t * \\mathrm{power} - 0.0025 * \\mathrm{cos}(3 * p_t)$$\n",
        "$$p_{t+1} = p_t + v_{t+1}$$\n",
        "(where $a_t$ is the current action, $p_t$ the current position, $v_t$ the current velocity, and $\\mathrm{power}=0.0015$)\n",
        "- the episode terminates if the the car reaches the goal or timestep 999 is reached"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8b75e6-996b-41b0-bec6-36bab9477f97",
      "metadata": {
        "id": "ab8b75e6-996b-41b0-bec6-36bab9477f97"
      },
      "outputs": [],
      "source": [
        "class MountainCar:\n",
        "  def __init__(self):\n",
        "    self.min_pos = -1.2\n",
        "    self.max_pos = 0.6\n",
        "    self.min_v = -0.07\n",
        "    self.max_v = 0.07\n",
        "    self.power = 0.0015\n",
        "    self.goal_pos = 0.45\n",
        "    self.min_action = -1\n",
        "    self.max_action = 1\n",
        "    self.timestamp = 0\n",
        "    self.state = None\n",
        "\n",
        "  def reset(self, seed=None):\n",
        "    # prepare for another episode\n",
        "    seed_seq = np.random.SeedSequence(seed)\n",
        "    self.rng = np.random.Generator(np.random.PCG64(seed_seq))  # using rng for sampling\n",
        "    pos = self.rng.uniform(-0.6,-0.4)\n",
        "    v = 0\n",
        "    self.state = np.array([pos, v])\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    # performing single environment step\n",
        "    pos, v = self.state\n",
        "    force = min(max(action[0],self.min_action), self.max_action)\n",
        "    v = v + (force*self.power) - 0.0025*np.cos(3*pos)\n",
        "    v = np.clip(v, self.min_v, self.max_v)\n",
        "    pos = pos + v\n",
        "    pos = np.clip(pos, self.min_pos, self.max_pos)\n",
        "    if pos == self.min_pos and v < 0:\n",
        "      v = 0\n",
        "    self.timestamp = self.timestamp +1\n",
        "    done = bool(pos>=self.goal_pos or self.timestamp ==999)\n",
        "    reward = 0\n",
        "    if done:\n",
        "      reward = 100\n",
        "    reward = reward - 0.1*np.power(action[0],2)\n",
        "    self.state = np.array([pos,v])\n",
        "    return self.state, reward, done"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466f4b9c-cfce-4d93-ae35-e4020d7f27bb",
      "metadata": {
        "id": "466f4b9c-cfce-4d93-ae35-e4020d7f27bb"
      },
      "source": [
        "## testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2c9569-e709-40ad-a9fd-16c13a65878f",
      "metadata": {
        "id": "1c2c9569-e709-40ad-a9fd-16c13a65878f"
      },
      "source": [
        "We can check created MountainCar environment by seeding it (with 12345) and comparing it to the gymnasium implementation (that is seeded to the same random seed) on the provided action sequence (should yield the same state and reward sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c84956e-e8e3-489f-aa93-7b6e9061719e",
      "metadata": {
        "id": "8c84956e-e8e3-489f-aa93-7b6e9061719e"
      },
      "outputs": [],
      "source": [
        "env_real = gym.make(\"MountainCarContinuous-v0\",render_mode=\"rgb_array\")\n",
        "action_seq = np.load(\"action_seq.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a2a2b9-bfb1-45f1-bcaf-6aa55d742b6c",
      "metadata": {
        "id": "23a2a2b9-bfb1-45f1-bcaf-6aa55d742b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5e0c73-0c5f-402e-a1ac-2d2c84d26fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.55453277  0.        ] [-0.5558011  -0.00126833]\n",
            "[-0.5558011  -0.00126833] [-0.5583283  -0.00252719]\n",
            "[-0.5583283  -0.00252719] [-0.56209546 -0.00376719]\n",
            "[-0.56209546 -0.00376719] [-0.56707454 -0.0049791 ]\n",
            "[-0.56707454 -0.0049791 ] [-0.5732285  -0.00615396]\n",
            "[-0.5732285  -0.00615396] [-0.58051157 -0.00728311]\n",
            "[-0.58051157 -0.00728311] [-0.5888699  -0.00835833]\n",
            "[-0.5888699  -0.00835833] [-0.59824187 -0.00937192]\n",
            "[-0.59824187 -0.00937192] [-0.60855865 -0.01031676]\n",
            "[-0.60855865 -0.01031676] [-0.6197451  -0.01118644]\n",
            "[-0.6197451  -0.01118644] [-0.63172036 -0.0119753 ]\n",
            "[-0.63172036 -0.0119753 ] [-0.6443989  -0.01267854]\n",
            "[-0.6443989  -0.01267854] [-0.6576912  -0.01329224]\n",
            "[-0.6576912  -0.01329224] [-0.6715046  -0.01381345]\n",
            "[-0.6715046  -0.01381345] [-0.6857448 -0.0142402]\n",
            "[-0.6857448 -0.0142402] [-0.7003163 -0.0145715]\n",
            "[-0.7003163 -0.0145715] [-0.71512365 -0.01480734]\n",
            "[-0.71512365 -0.01480734] [-0.7300723  -0.01494864]\n",
            "[-0.7300723  -0.01494864] [-0.74506956 -0.01499723]\n",
            "[-0.74506956 -0.01499723] [-0.7600253  -0.01495574]\n",
            "[-0.7600253  -0.01495574] [-0.7748529  -0.01482752]\n",
            "[-0.7748529  -0.01482752] [-0.7894694  -0.01461656]\n",
            "[-0.7894694  -0.01461656] [-0.80379677 -0.01432733]\n",
            "[-0.80379677 -0.01432733] [-0.8177615  -0.01396473]\n",
            "[-0.8177615  -0.01396473] [-0.83129543 -0.01353393]\n",
            "[-0.83129543 -0.01353393] [-0.8443357  -0.01304025]\n",
            "[-0.8443357  -0.01304025] [-0.85682476 -0.01248911]\n",
            "[-0.85682476 -0.01248911] [-0.86871064 -0.01188587]\n",
            "[-0.86871064 -0.01188587] [-0.8799464  -0.01123578]\n",
            "[-0.8799464  -0.01123578] [-0.89049035 -0.01054393]\n",
            "[-0.89049035 -0.01054393] [-0.9003055  -0.00981515]\n",
            "[-0.9003055  -0.00981515] [-0.9093595  -0.00905399]\n",
            "[-0.9093595  -0.00905399] [-0.91762424 -0.0082647 ]\n",
            "[-0.91762424 -0.0082647 ] [-0.9250755  -0.00745122]\n",
            "[-0.9250755  -0.00745122] [-0.9316926  -0.00661713]\n",
            "[-0.9316926  -0.00661713] [-0.93745834 -0.00576572]\n",
            "[-0.93745834 -0.00576572] [-0.9423583  -0.00489998]\n",
            "[-0.9423583  -0.00489998] [-0.9463809  -0.00402262]\n",
            "[-0.9463809  -0.00402262] [-0.949517  -0.0031361]\n",
            "[-0.949517  -0.0031361] [-0.9517597  -0.00224267]\n",
            "[-0.9517597  -0.00224267] [-0.95310414 -0.00134444]\n",
            "[-0.95310414 -0.00134444] [-9.5354754e-01 -4.4338612e-04]\n",
            "[-9.5354754e-01 -4.4338612e-04] [-0.9500889  0.0034586]\n",
            "[-0.9500889  0.0034586] [-0.9427357   0.00735326]\n",
            "[-0.9427357   0.00735326] [-0.9315042   0.01123149]\n",
            "[-0.9315042   0.01123149] [-0.9164218   0.01508242]\n",
            "[-0.9164218   0.01508242] [-0.8975293   0.01889247]\n",
            "[-0.8975293   0.01889247] [-0.8748846   0.02264467]\n",
            "[-0.8748846   0.02264467] [-0.8485666   0.02631801]\n",
            "[-0.8485666   0.02631801] [-0.81867945  0.02988713]\n",
            "[-0.81867945  0.02988713] [-0.7853572  0.0333223]\n",
            "[-0.7853572  0.0333223] [-0.7487673   0.03658985]\n",
            "[-0.7487673   0.03658985] [-0.70911425  0.03965308]\n",
            "[-0.70911425  0.03965308] [-0.6666405   0.04247372]\n",
            "[-0.6666405   0.04247372] [-0.6216266   0.04501391]\n",
            "[-0.6216266   0.04501391] [-0.574388    0.04723857]\n",
            "[-0.574388    0.04723857] [-0.52527     0.04911802]\n",
            "[-0.52527     0.04911802] [-0.47463945  0.05063055]\n",
            "[-0.47463945  0.05063055] [-0.42287478  0.05176467]\n",
            "[-0.42287478  0.05176467] [-0.3703541   0.05252069]\n",
            "[-0.3703541   0.05252069] [-0.3174427   0.05291142]\n",
            "[-0.3174427   0.05291142] [-0.26448074  0.05296195]\n",
            "[-0.26448074  0.05296195] [-0.21177228  0.05270845]\n",
            "[-0.21177228  0.05270845] [-0.15957603  0.05219625]\n",
            "[-0.15957603  0.05219625] [-0.10809874  0.05147729]\n",
            "[-0.10809874  0.05147729] [-0.05749113  0.0506076 ]\n",
            "[-0.05749113  0.0506076 ] [-0.00784644  0.04964469]\n",
            "[-0.00784644  0.04964469] [0.04079895 0.04864538]\n",
            "[0.04079895 0.04864538] [0.08846304 0.04766409]\n",
            "[0.08846304 0.04766409] [0.13521466 0.04675161]\n",
            "[0.13521466 0.04675161] [0.18116915 0.04595449]\n",
            "[0.18116915 0.04595449] [0.2264839  0.04531474]\n",
            "[0.2264839  0.04531474] [0.27135384 0.04486994]\n",
            "[0.27135384 0.04486994] [0.3160074  0.04465357]\n",
            "[0.3160074  0.04465357] [0.36070275 0.04469534]\n",
            "[0.36070275 0.04469534] [0.40572444 0.04502167]\n",
            "[0.40572444 0.04502167] [0.45138037 0.04565592]\n"
          ]
        }
      ],
      "source": [
        "# compare gym and your environment - they should yield the same state sequence\n",
        "state, _ = env_real.reset(seed=12345)\n",
        "for action in action_seq:\n",
        "  next_state, reward, terminated, truncated, _ = env_real.step(action)\n",
        "  print(state, next_state)\n",
        "  if terminated or truncated:\n",
        "    break\n",
        "  state = next_state\n",
        "env_real.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_mc = MountainCar()\n",
        "state = env_mc.reset(seed=12345)\n",
        "for action in action_seq:\n",
        "  next_state, reward, done = env_mc.step(action)\n",
        "  print(state, next_state)\n",
        "  if done:\n",
        "    break\n",
        "  state = next_state"
      ],
      "metadata": {
        "id": "aVaA0t_7Bd1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d508fc-67ff-41be-faec-29c53ba7c921"
      },
      "id": "aVaA0t_7Bd1-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.5545328  0.       ] [-0.55580112 -0.00126833]\n",
            "[-0.55580112 -0.00126833] [-0.55832831 -0.00252719]\n",
            "[-0.55832831 -0.00252719] [-0.56209549 -0.00376719]\n",
            "[-0.56209549 -0.00376719] [-0.5670746 -0.0049791]\n",
            "[-0.5670746 -0.0049791] [-0.57322855 -0.00615396]\n",
            "[-0.57322855 -0.00615396] [-0.58051166 -0.00728311]\n",
            "[-0.58051166 -0.00728311] [-0.58886999 -0.00835833]\n",
            "[-0.58886999 -0.00835833] [-0.59824191 -0.00937192]\n",
            "[-0.59824191 -0.00937192] [-0.60855867 -0.01031676]\n",
            "[-0.60855867 -0.01031676] [-0.61974511 -0.01118644]\n",
            "[-0.61974511 -0.01118644] [-0.63172041 -0.0119753 ]\n",
            "[-0.63172041 -0.0119753 ] [-0.64439894 -0.01267853]\n",
            "[-0.64439894 -0.01267853] [-0.65769118 -0.01329223]\n",
            "[-0.65769118 -0.01329223] [-0.67150462 -0.01381345]\n",
            "[-0.67150462 -0.01381345] [-0.68574482 -0.0142402 ]\n",
            "[-0.68574482 -0.0142402 ] [-0.70031631 -0.0145715 ]\n",
            "[-0.70031631 -0.0145715 ] [-0.71512365 -0.01480733]\n",
            "[-0.71512365 -0.01480733] [-0.73007229 -0.01494864]\n",
            "[-0.73007229 -0.01494864] [-0.74506952 -0.01499723]\n",
            "[-0.74506952 -0.01499723] [-0.76002526 -0.01495574]\n",
            "[-0.76002526 -0.01495574] [-0.77485278 -0.01482752]\n",
            "[-0.77485278 -0.01482752] [-0.78946933 -0.01461655]\n",
            "[-0.78946933 -0.01461655] [-0.80379666 -0.01432733]\n",
            "[-0.80379666 -0.01432733] [-0.81776139 -0.01396473]\n",
            "[-0.81776139 -0.01396473] [-0.83129532 -0.01353393]\n",
            "[-0.83129532 -0.01353393] [-0.84433557 -0.01304025]\n",
            "[-0.84433557 -0.01304025] [-0.85682469 -0.01248911]\n",
            "[-0.85682469 -0.01248911] [-0.86871055 -0.01188587]\n",
            "[-0.86871055 -0.01188587] [-0.87994634 -0.01123578]\n",
            "[-0.87994634 -0.01123578] [-0.89049027 -0.01054393]\n",
            "[-0.89049027 -0.01054393] [-0.90030542 -0.00981515]\n",
            "[-0.90030542 -0.00981515] [-0.90935941 -0.00905399]\n",
            "[-0.90935941 -0.00905399] [-0.91762412 -0.00826471]\n",
            "[-0.91762412 -0.00826471] [-0.92507534 -0.00745122]\n",
            "[-0.92507534 -0.00745122] [-0.93169247 -0.00661713]\n",
            "[-0.93169247 -0.00661713] [-0.9374582  -0.00576573]\n",
            "[-0.9374582  -0.00576573] [-0.94235819 -0.00489999]\n",
            "[-0.94235819 -0.00489999] [-0.94638081 -0.00402262]\n",
            "[-0.94638081 -0.00402262] [-0.94951691 -0.0031361 ]\n",
            "[-0.94951691 -0.0031361 ] [-0.95175959 -0.00224268]\n",
            "[-0.95175959 -0.00224268] [-0.95310404 -0.00134445]\n",
            "[-0.95310404 -0.00134445] [-9.53547428e-01 -4.43391284e-04]\n",
            "[-9.53547428e-01 -4.43391284e-04] [-0.95008884  0.00345859]\n",
            "[-0.95008884  0.00345859] [-0.94273559  0.00735325]\n",
            "[-0.94273559  0.00735325] [-0.9315041   0.01123149]\n",
            "[-0.9315041   0.01123149] [-0.91642169  0.01508241]\n",
            "[-0.91642169  0.01508241] [-0.89752922  0.01889247]\n",
            "[-0.89752922  0.01889247] [-0.87488456  0.02264466]\n",
            "[-0.87488456  0.02264466] [-0.84856655  0.026318  ]\n",
            "[-0.84856655  0.026318  ] [-0.81867943  0.02988712]\n",
            "[-0.81867943  0.02988712] [-0.78535714  0.03332229]\n",
            "[-0.78535714  0.03332229] [-0.74876729  0.03658984]\n",
            "[-0.74876729  0.03658984] [-0.70911422  0.03965307]\n",
            "[-0.70911422  0.03965307] [-0.6666405   0.04247372]\n",
            "[-0.6666405   0.04247372] [-0.6216266  0.0450139]\n",
            "[-0.6216266  0.0450139] [-0.57438804  0.04723856]\n",
            "[-0.57438804  0.04723856] [-0.52527003  0.04911801]\n",
            "[-0.52527003  0.04911801] [-0.47463949  0.05063054]\n",
            "[-0.47463949  0.05063054] [-0.42287482  0.05176467]\n",
            "[-0.42287482  0.05176467] [-0.37035414  0.05252068]\n",
            "[-0.37035414  0.05252068] [-0.31744273  0.05291141]\n",
            "[-0.31744273  0.05291141] [-0.26448079  0.05296194]\n",
            "[-0.26448079  0.05296194] [-0.21177234  0.05270845]\n",
            "[-0.21177234  0.05270845] [-0.15957611  0.05219624]\n",
            "[-0.15957611  0.05219624] [-0.10809882  0.05147729]\n",
            "[-0.10809882  0.05147729] [-0.05749122  0.0506076 ]\n",
            "[-0.05749122  0.0506076 ] [-0.00784653  0.04964469]\n",
            "[-0.00784653  0.04964469] [0.04079885 0.04864538]\n",
            "[0.04079885 0.04864538] [0.08846293 0.04766408]\n",
            "[0.08846293 0.04766408] [0.13521454 0.04675161]\n",
            "[0.13521454 0.04675161] [0.18116903 0.04595449]\n",
            "[0.18116903 0.04595449] [0.22648377 0.04531474]\n",
            "[0.22648377 0.04531474] [0.27135371 0.04486994]\n",
            "[0.27135371 0.04486994] [0.31600727 0.04465356]\n",
            "[0.31600727 0.04465356] [0.36070261 0.04469534]\n",
            "[0.36070261 0.04469534] [0.40572427 0.04502167]\n",
            "[0.40572427 0.04502167] [0.45138019 0.04565592]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df405689-818e-47bc-8e71-91749eda48e5",
      "metadata": {
        "id": "df405689-818e-47bc-8e71-91749eda48e5"
      },
      "source": [
        "## Model-based RL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c5b713f-cb86-4062-a08e-27bc4b927f49",
      "metadata": {
        "id": "5c5b713f-cb86-4062-a08e-27bc4b927f49"
      },
      "source": [
        "Now we try and solve this MDP - we opt for a model-based RL algorithm, meaning that we use an explicitly trained model of the transition dynamics. To this end, we collect data from the environment in the form of $(s,a,s',r)$ tuples, train the surrogate model $\\hat{T}$ to replicate the state transition dynamics and later use it to derive a policy $\\pi:S \\rightarrow A$, such that we reach the goal to maximize the return.\n",
        "\n",
        "In this section we start by training a dynamics model in torch - it is recommended to train one model for each of the state dimensions and predict differences instead of absolute values (i.e. predict $\\Delta v_t = v_{t+1} - v_t$ instead of $v_{t+1}$ directly).\n",
        "\n",
        "* Since the environment is rather simple, roughly 10,000 interactions should suffice for data collection and 3 hidden layers of size 10 should be enough to capture the transition function.  \n",
        "* Uniform random actions during collection and stop trajectories after 200 steps.\n",
        "* Using the option of the gym environment to sample starting states uniformly in the whole range in $[-1.2, 0.6]$ for data collection (using only the standard range will require much more data collection to obtain accurate models since the edge regions are hard to reach under random actions).\n",
        "* Splitting our data into 90 / 10 percent for training and validation.\n",
        "* Expected an MSE validation error below $1e-6$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa531da7-4f16-45a6-868d-bb1eef8d930e",
      "metadata": {
        "id": "aa531da7-4f16-45a6-868d-bb1eef8d930e"
      },
      "source": [
        "## define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42efd2f-dd43-46df-aa8e-3e0350e08a43",
      "metadata": {
        "id": "c42efd2f-dd43-46df-aa8e-3e0350e08a43"
      },
      "outputs": [],
      "source": [
        "class TransitionModel(torch.nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, hidden_dims):\n",
        "    super(TransitionModel,self).__init__()\n",
        "    self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dims[0])\n",
        "    self.fc2 = torch.nn.Linear(hidden_dims[0],hidden_dims[1])\n",
        "    self.fc3 = torch.nn.Linear(hidden_dims[1],hidden_dims[2])\n",
        "    self.fc4 = torch.nn.Linear(hidden_dims[2], state_dim)\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    x = torch.cat((state,action), dim=1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    return self.fc4(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c4ba633-d1e5-437c-8178-ed361af45e9f",
      "metadata": {
        "id": "1c4ba633-d1e5-437c-8178-ed361af45e9f"
      },
      "source": [
        "## collect data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collect data for training\n",
        "env = gym.make(\"MountainCarContinuous-v0\")\n",
        "num_interactions = 50\n",
        "max_steps = 200\n",
        "transitions = []\n",
        "\n",
        "for _ in range(num_interactions):\n",
        "  state, _ = env.reset(seed=12345)\n",
        "  t = {'states':[], 'actions':[], 'next_states':[],'rewards':[]}\n",
        "  for _ in range(max_steps):\n",
        "    action = env.action_space.sample()\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    t['states'].append(state)\n",
        "    t['actions'].append(action[0])\n",
        "    t['next_states'].append(next_state)\n",
        "    t['rewards'].append(reward)\n",
        "    if terminated or truncated:\n",
        "      break\n",
        "    state = next_state\n",
        "  transitions.append(t)\n",
        "  train_size = int(0.9 * num_interactions)\n",
        "  train = transitions[:train_size]\n",
        "  valid = transitions[train_size:]\n"
      ],
      "metadata": {
        "id": "fgJ0ZmLGzY5h"
      },
      "id": "fgJ0ZmLGzY5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "29b8c641-6ddc-4958-a59b-ddcc4da096e1",
      "metadata": {
        "id": "29b8c641-6ddc-4958-a59b-ddcc4da096e1"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "hidden_dims = [10,10,10]\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "def prepare_data(data):\n",
        "  states = torch.tensor(data['states'], dtype=torch.float32)\n",
        "  actions = torch.tensor(data['actions'], dtype=torch.float32)\n",
        "  actions = torch.reshape(actions, (len(actions),1))\n",
        "  next_states = torch.tensor(data['next_states'], dtype=torch.float32)\n",
        "  #r = torch.tensor(data['rewards'], dtype=torch.float32)\n",
        "  #rewards = torch.reshape(r, (len(r),1))\n",
        "  return states, actions, next_states #, rewards"
      ],
      "metadata": {
        "id": "IfET68_AhPRd"
      },
      "id": "IfET68_AhPRd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training a model for the position variable *p*"
      ],
      "metadata": {
        "id": "MxzObyFLoORQ"
      },
      "id": "MxzObyFLoORQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate transition models and train them\n",
        "pModel = TransitionModel(state_dim, action_dim, hidden_dims)\n",
        "optimizer = torch.optim.Adam(pModel.parameters(), lr=0.001)\n",
        "for e in range(10):\n",
        "  pModel.train()\n",
        "  train_loss = 0\n",
        "  for t in train:\n",
        "    s, a, ns = prepare_data(t)\n",
        "    optimizer.zero_grad()\n",
        "    pred = pModel(s, a)\n",
        "    loss = criterion(pred[:,:1], ns[:,:1]-s[:,:1])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "  pModel.eval()\n",
        "  val_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for v in valid:\n",
        "      s, a, ns = prepare_data(v)\n",
        "      pred = pModel(s,a)\n",
        "      loss = criterion(pred[:,:1], ns[:,:1]-s[:,:1])\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  print(f'Epoch [{e + 1}/{10}], Training Loss: {train_loss / len(train)}, Validation Loss:{val_loss/len(valid)}')\n"
      ],
      "metadata": {
        "id": "L1qbk2xqEbNF"
      },
      "id": "L1qbk2xqEbNF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training another model for velocity *v*"
      ],
      "metadata": {
        "id": "6xYYyx5Voc9w"
      },
      "id": "6xYYyx5Voc9w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9d63226-03b9-42e6-bde2-7dc4a04487b0",
      "metadata": {
        "id": "a9d63226-03b9-42e6-bde2-7dc4a04487b0"
      },
      "outputs": [],
      "source": [
        "vModel = TransitionModel(state_dim, action_dim, hidden_dims)\n",
        "optimizer = torch.optim.Adam(vModel.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  vModel.train()\n",
        "  total_loss = 0\n",
        "  for t in train:\n",
        "    s, a, ns = prepare_data(t)\n",
        "    optimizer.zero_grad()\n",
        "    pred = vModel(s, a)\n",
        "    loss = criterion(pred[:,:2], ns[:,:2]-s[:,:2])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss +=loss.item()\n",
        "  vModel.eval()\n",
        "  val_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for v in valid:\n",
        "      s, a, ns = prepare_data(v)\n",
        "      pred = vModel(s,a)\n",
        "      loss = criterion(pred[:,:2], ns[:,:2]-s[:,:2])\n",
        "      val_loss += loss.item()\n",
        "  print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {total_loss / len(train)}, Validation Loss:{val_loss/len(valid)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving models"
      ],
      "metadata": {
        "id": "JyydRGRoond5"
      },
      "id": "JyydRGRoond5"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(pModel, 'pModel.pt')\n",
        "torch.save(vModel, 'vModel.pt')\n",
        "#pModel = torch.load('pModel.pt')\n",
        "#vModel = torch.load('vModel.pt')"
      ],
      "metadata": {
        "id": "er79YRHXF-1p"
      },
      "id": "er79YRHXF-1p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "111629c3-c1e6-4beb-aff1-ad49db50dd74",
      "metadata": {
        "id": "111629c3-c1e6-4beb-aff1-ad49db50dd74"
      },
      "source": [
        "## Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184fd82c-512b-450f-92bd-5117bf40cebb",
      "metadata": {
        "id": "184fd82c-512b-450f-92bd-5117bf40cebb"
      },
      "source": [
        "To check, we first build a compound model that encapsulates both predictions into one, i.e. it takes in state and action and returns a full state consisting of both position and velocity.\n",
        "\n",
        "We use the simple_policy to collect episodes in the real environment & use the same starting state and actions to collect a \"virtual\" / \"imagined\" episode. They need not be identical since our model cannot be perfect (however they should be quite similar and both should reach the goal within ~100 steps with the simple policy)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce284b4-9168-4772-9693-16418d4f4143",
      "metadata": {
        "id": "6ce284b4-9168-4772-9693-16418d4f4143"
      },
      "source": [
        "## compound model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dda020f-00c8-4c57-a206-49b90d32ddee",
      "metadata": {
        "id": "8dda020f-00c8-4c57-a206-49b90d32ddee"
      },
      "outputs": [],
      "source": [
        "class CompoundModel:\n",
        "  def __init__(self, pModel, vModel):\n",
        "    self.pModel = pModel\n",
        "    self.vModel = vModel\n",
        "\n",
        "  def predict(self, state, action, return_torch=False):\n",
        "    a = torch.FloatTensor(action)\n",
        "    print(len(a))\n",
        "    ac = torch.reshape(a, (len(a),1))\n",
        "    s = torch.FloatTensor(state)\n",
        "    print(len(s))\n",
        "    st = torch.reshape(s, (len(s),2))\n",
        "    delta_pos = self.pModel(st, ac)\n",
        "    delta_vel = self.vModel(st, ac)\n",
        "    next_pos = state[0] + delta_pos.data[0][0].numpy()\n",
        "    next_vel = state[1] + delta_vel.data[0][1].numpy()\n",
        "    next_state_numpy = np.array([next_pos, next_vel])\n",
        "    return next_state_numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf5a078-d6c4-4c4c-8c92-b53070f2b35b",
      "metadata": {
        "id": "edf5a078-d6c4-4c4c-8c92-b53070f2b35b"
      },
      "source": [
        "## test in trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d804571-f1a4-48e5-b482-1ec9ac986b1c",
      "metadata": {
        "id": "6d804571-f1a4-48e5-b482-1ec9ac986b1c"
      },
      "outputs": [],
      "source": [
        "simple_policy = lambda x: np.array([1.]) if x[1] > 0. else np.array([-1.])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad31990a-5e52-4ead-9653-e20fa294b8ab",
      "metadata": {
        "scrolled": true,
        "id": "ad31990a-5e52-4ead-9653-e20fa294b8ab"
      },
      "outputs": [],
      "source": [
        "# use the gym env to compare state sequences obtained by environment and surrogate model when executing the same actions\n",
        "env = gym.make(\"MountainCarContinuous-v0\")\n",
        "model = CompoundModel(pModel, vModel)\n",
        "goal_position = 0.45\n",
        "s, _ = env.reset(seed=42)\n",
        "state = s\n",
        "\n",
        "for _ in range(100):\n",
        "  action = simple_policy(state)\n",
        "  next_state = model.predict(state, action)\n",
        "  print(state, next_state)\n",
        "  done = bool(next_state[0] >= goal_position)\n",
        "  if done:\n",
        "    print(\"Terminated\")\n",
        "    break\n",
        "  state = next_state"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = s\n",
        "for _ in range(100):\n",
        "  action = simple_policy(state)\n",
        "  next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "  print(state, next_state)\n",
        "  if terminated or truncated:\n",
        "    print(\"Terminated\")\n",
        "    break\n",
        "  state = next_state\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnID0PUI9qn9",
        "outputId": "07b69fcf-cba2-4031-b4d6-12a73c703284"
      },
      "id": "hnID0PUI9qn9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.4452088  0.       ] [-0.4472913  -0.00208252]\n",
            "[-0.4472913  -0.00208252] [-0.45144117 -0.00414984]\n",
            "[-0.45144117 -0.00414984] [-0.45762798 -0.00618681]\n",
            "[-0.45762798 -0.00618681] [-0.46580634 -0.00817836]\n",
            "[-0.46580634 -0.00817836] [-0.47591597 -0.01010964]\n",
            "[-0.47591597 -0.01010964] [-0.48788202 -0.01196604]\n",
            "[-0.48788202 -0.01196604] [-0.5016154 -0.0137334]\n",
            "[-0.5016154 -0.0137334] [-0.51701355 -0.01539816]\n",
            "[-0.51701355 -0.01539816] [-0.5339611  -0.01694755]\n",
            "[-0.5339611  -0.01694755] [-0.552331   -0.01836984]\n",
            "[-0.552331   -0.01836984] [-0.5719856  -0.01965462]\n",
            "[-0.5719856  -0.01965462] [-0.59277856 -0.02079299]\n",
            "[-0.59277856 -0.02079299] [-0.61455643 -0.02177786]\n",
            "[-0.61455643 -0.02177786] [-0.63716054 -0.02260411]\n",
            "[-0.63716054 -0.02260411] [-0.6604293  -0.02326878]\n",
            "[-0.6604293  -0.02326878] [-0.6842004  -0.02377113]\n",
            "[-0.6842004  -0.02377113] [-0.7083131  -0.02411268]\n",
            "[-0.7083131  -0.02411268] [-0.7326102  -0.02429714]\n",
            "[-0.7326102  -0.02429714] [-0.7569405  -0.02433028]\n",
            "[-0.7569405  -0.02433028] [-0.7811602  -0.02421968]\n",
            "[-0.7811602  -0.02421968] [-0.8051347  -0.02397453]\n",
            "[-0.8051347  -0.02397453] [-0.82873994 -0.02360526]\n",
            "[-0.82873994 -0.02360526] [-0.85186315 -0.0231232 ]\n",
            "[-0.85186315 -0.0231232 ] [-0.8744035  -0.02254031]\n",
            "[-0.8744035  -0.02254031] [-0.89627224 -0.02186876]\n",
            "[-0.89627224 -0.02186876] [-0.9173929  -0.02112067]\n",
            "[-0.9173929  -0.02112067] [-0.93770075 -0.02030784]\n",
            "[-0.93770075 -0.02030784] [-0.95714223 -0.01944151]\n",
            "[-0.95714223 -0.01944151] [-0.97567445 -0.01853219]\n",
            "[-0.97567445 -0.01853219] [-0.99326396 -0.01758952]\n",
            "[-0.99326396 -0.01758952] [-1.0098861  -0.01662218]\n",
            "[-1.0098861  -0.01662218] [-1.025524   -0.01563782]\n",
            "[-1.025524   -0.01563782] [-1.0401671  -0.01464311]\n",
            "[-1.0401671  -0.01464311] [-1.0538107  -0.01364366]\n",
            "[-1.0538107  -0.01364366] [-1.0664549  -0.01264415]\n",
            "[-1.0664549  -0.01264415] [-1.0781032  -0.01164833]\n",
            "[-1.0781032  -0.01164833] [-1.0887623  -0.01065906]\n",
            "[-1.0887623  -0.01065906] [-1.0984408  -0.00967847]\n",
            "[-1.0984408  -0.00967847] [-1.1071488  -0.00870796]\n",
            "[-1.1071488  -0.00870796] [-1.114897   -0.00774828]\n",
            "[-1.114897   -0.00774828] [-1.1216967  -0.00679967]\n",
            "[-1.1216967  -0.00679967] [-1.1275586  -0.00586185]\n",
            "[-1.1275586  -0.00586185] [-1.1324928  -0.00493415]\n",
            "[-1.1324928  -0.00493415] [-1.1365083  -0.00401555]\n",
            "[-1.1365083  -0.00401555] [-1.139613   -0.00310475]\n",
            "[-1.139613   -0.00310475] [-1.1418133  -0.00220021]\n",
            "[-1.1418133  -0.00220021] [-1.1431135  -0.00130025]\n",
            "[-1.1431135  -0.00130025] [-1.1435165e+00 -4.0303747e-04]\n",
            "[-1.1435165e+00 -4.0303747e-04] [-1.1430233e+00  4.9331656e-04]\n",
            "[-1.1430233e+00  4.9331656e-04] [-1.1386325   0.00439072]\n",
            "[-1.1386325   0.00439072] [-1.1303353   0.00829726]\n",
            "[-1.1303353   0.00829726] [-1.1181154  0.0122199]\n",
            "[-1.1181154  0.0122199] [-1.1019518   0.01616353]\n",
            "[-1.1019518   0.01616353] [-1.0818219   0.02012988]\n",
            "[-1.0818219   0.02012988] [-1.0577055  0.0241164]\n",
            "[-1.0577055  0.0241164] [-1.0295904   0.02811516]\n",
            "[-1.0295904   0.02811516] [-0.99747866  0.03211167]\n",
            "[-0.99747866  0.03211167] [-0.9613947   0.03608392]\n",
            "[-0.9613947   0.03608392] [-0.92139316  0.04000155]\n",
            "[-0.92139316  0.04000155] [-0.8775675  0.0438256]\n",
            "[-0.8775675  0.0438256] [-0.8300587   0.04750881]\n",
            "[-0.8300587   0.04750881] [-0.7790618   0.05099688]\n",
            "[-0.7790618   0.05099688] [-0.72483104  0.05423072]\n",
            "[-0.72483104  0.05423072] [-0.6676811   0.05714995]\n",
            "[-0.6676811   0.05714995] [-0.6079839   0.05969723]\n",
            "[-0.6079839   0.05969723] [-0.5461605   0.06182338]\n",
            "[-0.5461605   0.06182338] [-0.48266807  0.06349246]\n",
            "[-0.48266807  0.06349246] [-0.41798183  0.06468625]\n",
            "[-0.41798183  0.06468625] [-0.35257453  0.06540731]\n",
            "[-0.35257453  0.06540731] [-0.28689435  0.06568017]\n",
            "[-0.28689435  0.06568017] [-0.22134398  0.06555037]\n",
            "[-0.22134398  0.06555037] [-0.1562624   0.06508159]\n",
            "[-0.1562624   0.06508159] [-0.0919111  0.0643513]\n",
            "[-0.0919111  0.0643513] [-0.02846537  0.06344573]\n",
            "[-0.02846537  0.06344573] [0.03398948 0.06245484]\n",
            "[0.03398948 0.06245484] [0.09545731 0.06146783]\n",
            "[0.09545731 0.06146783] [0.15602694 0.06056964]\n",
            "[0.15602694 0.06056964] [0.2158655  0.05983855]\n",
            "[0.2158655  0.05983855] [0.2752102  0.05934471]\n",
            "[0.2752102  0.05934471] [0.33435968 0.05914948]\n",
            "[0.33435968 0.05914948] [0.39366487 0.05930521]\n",
            "[0.39366487 0.05930521] [0.45352006 0.05985519]\n",
            "Terminated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9817745f-0bff-4b64-adfc-db6ca917e3c4",
      "metadata": {
        "id": "9817745f-0bff-4b64-adfc-db6ca917e3c4"
      },
      "source": [
        "## Policy model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d781c533-15c3-4834-86e9-d5c109a2d3a9",
      "metadata": {
        "id": "d781c533-15c3-4834-86e9-d5c109a2d3a9"
      },
      "source": [
        "next, we want to use the trained transition model to derive a closed form policy $\\pi:S \\rightarrow A$ to solve the problem. First, define a neural network policy model in torch - a single layer of 20 hidden units should suffice for this simple problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb811163-efc7-4eb6-a911-ce7e90efc0fc",
      "metadata": {
        "id": "eb811163-efc7-4eb6-a911-ce7e90efc0fc"
      },
      "outputs": [],
      "source": [
        "class PolicyModel(torch.nn.Module):\n",
        "  def __init__(self, state_dim, action_dim, hidden_dim):\n",
        "    super(PolicyModel, self).__init__()\n",
        "    self.i = nn.Linear(state_dim, hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.i(state))\n",
        "    action = F.tanh(self.out(x))\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6546e61-e39a-4601-ae97-6ff891c94cba",
      "metadata": {
        "id": "f6546e61-e39a-4601-ae97-6ff891c94cba"
      },
      "source": [
        "## define rollouts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9f4bac-b4bb-45a3-b426-a1060e4d54b4",
      "metadata": {
        "id": "2b9f4bac-b4bb-45a3-b426-a1060e4d54b4"
      },
      "source": [
        "Next, we'll need a way to use the model to assess the policy's performance. To do so,\n",
        "\n",
        "* We sample start states from the from the dataset and perform imagined episodes from there on out by following the policy actions and using the dynamics model's state predictions.\n",
        "\n",
        "* We use our knowledge about the environment and directly declare any of the episodes that reach the goal as success.\n",
        "\n",
        "* The rollout function returns the ratio of episodes that resulted in success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d6cba8-7405-463d-9a66-6dedcd694d5c",
      "metadata": {
        "id": "44d6cba8-7405-463d-9a66-6dedcd694d5c"
      },
      "outputs": [],
      "source": [
        "def rollout(policy, dynamics, steps, start_states):\n",
        "  success_count = 0\n",
        "  for _ in range(steps):\n",
        "      actions = policy(start_states)\n",
        "      next_states = dynamics.predict(start_states, actions)\n",
        "      success_states = [ns for ns in next_states if ns[0]>=0.45]\n",
        "      success_count += len(success_states)\n",
        "      nstates = np.array([ns for ns in next_states if ns not in success_states])\n",
        "      start_states = torch.tensor(nstates, dtype=torch.float32)\n",
        "  success_ratio = success_count / len(start_states)\n",
        "  return success_ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "128cf7f6-6bc5-4209-9f5e-42e2bbaa7ac6",
      "metadata": {
        "id": "128cf7f6-6bc5-4209-9f5e-42e2bbaa7ac6"
      },
      "source": [
        "## policy training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de3c988-973f-43fe-85e5-d30b2d6c5a0a",
      "metadata": {
        "id": "8de3c988-973f-43fe-85e5-d30b2d6c5a0a"
      },
      "source": [
        "for the final policy training, we go for a gradient-free optimization technique, since gradient based approaches can exhibit unstable behavior in the model based setting with sparse rewards. For this we first need a fitness function, i.e. a function that receives the paramaters of the policy as a flat weight array in numpy and returns the fitness of this policy, i.e. its above defined success ratio:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sampleState():\n",
        "  pos = np.random.uniform(-1.2, 0.6, 1)\n",
        "  vel = 0.0\n",
        "  x = np.array([pos[0], vel])\n",
        "  return torch.reshape(torch.FloatTensor(x), (1,2))"
      ],
      "metadata": {
        "id": "THW7D1kwQ9h6"
      },
      "id": "THW7D1kwQ9h6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51bb6f8c-df7a-4ec6-a93c-89ce9ab8f840",
      "metadata": {
        "id": "51bb6f8c-df7a-4ec6-a93c-89ce9ab8f840"
      },
      "outputs": [],
      "source": [
        "def fitness(param_vec, batchsize=100, horizon=100):\n",
        "  state_dim = 2\n",
        "  action_dim = 1\n",
        "  hidden_dim = 20\n",
        "  start_states = torch.empty((0, 2), dtype=torch.float32)\n",
        "  for _ in range(batchsize):\n",
        "    state = sampleState()\n",
        "    print(state.size())\n",
        "    print(start_states.size())\n",
        "    start_states = torch.cat((start_states, state), dim=0)\n",
        "  policy = PolicyModel(state_dim, action_dim, hidden_dim)\n",
        "  stateDict = policy.state_dict()\n",
        "  for weight, p in zip(policy.state_dict(), param_vec):\n",
        "    stateDict[f\"{weight}\"] = p\n",
        "  dynamics = CompoundModel(pModel,vModel)\n",
        "  fitness = rollout(policy, dynamics, horizon, torch.as_tensor(start_states))\n",
        "  return fitness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8409a03d-5439-4034-9b5b-0be304c7c389",
      "metadata": {
        "id": "8409a03d-5439-4034-9b5b-0be304c7c389"
      },
      "source": [
        "finally, use the nevergrad library (https://facebookresearch.github.io/nevergrad/getting_started.html) to find well performing weights. Note: A budget of 5000 and rollouts of length 100 is usually plenty to derive a good policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3910c526-5459-4e1e-ad34-a3698944cec0",
      "metadata": {
        "id": "3910c526-5459-4e1e-ad34-a3698944cec0"
      },
      "outputs": [],
      "source": [
        "# train policy using nevergrad\n",
        "w1=ng.p.Array(shape=(20,2)),\n",
        "b1=ng.p.Array(shape=(20,)),\n",
        "w2=ng.p.Array(shape=(1,20)),\n",
        "b2=ng.p.Array(shape=(1,))\n",
        "params= ng.p.Tuple(w1,b1,w2,b2)\n",
        "instru = ng.p.Instrumentation(param_vec=params)\n",
        "optimizer = ng.optimizers.NGOpt(parametrization=instru, budget=500)\n",
        "recommendation = optimizer.minimize(fitness)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"policy.npy\", recommendation.value[1])"
      ],
      "metadata": {
        "id": "7gi-PYQ3NTvq"
      },
      "id": "7gi-PYQ3NTvq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def params_from_ng(recommendation):\n",
        "  w = np.load(recommendation, allow_pickle=True)\n",
        "  params = []\n",
        "  for i in w.item()['param_vec']:\n",
        "    if(type(i)==tuple):\n",
        "      x = np.array(i[0].value)\n",
        "      x = torch.tensor(x, dtype=torch.float32)\n",
        "    else:\n",
        "      x = np.array(i[0])\n",
        "      x = torch.tensor(x, dtype=torch.float32)\n",
        "      x = torch.reshape(x, (1,))\n",
        "    params.append(x)\n",
        "  return params"
      ],
      "metadata": {
        "id": "N9Dp-3jDOB78"
      },
      "id": "N9Dp-3jDOB78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1e47021f-f9cf-4ff6-af85-f549b7501548",
      "metadata": {
        "id": "1e47021f-f9cf-4ff6-af85-f549b7501548"
      },
      "source": [
        "## test policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20187481-21b7-4ea9-8132-ab7405b15bc8",
      "metadata": {
        "id": "20187481-21b7-4ea9-8132-ab7405b15bc8"
      },
      "source": [
        "We can test whether the policy that you found with nevergrad actually performs well in the real environment too - note that we trained on a surrogate of the environment that is not perfect. We use the policy to make at least 100 episodes of length 100 to get a decent estimate of the performance."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_weights(input_dim, hidden_units, output_dim):\n",
        "    W1 = 2 * torch.rand(hidden_units, input_dim) - 1\n",
        "    b1 = 2 * torch.rand(hidden_units) - 1\n",
        "    W2 = 2 * torch.rand(output_dim, hidden_units) - 1\n",
        "    b2 = 2 * torch.rand(output_dim) - 1\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "qGuPoPs2YAJy"
      },
      "id": "qGuPoPs2YAJy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = sample_weights(2,20,1)\n",
        "for i in x:\n",
        "  print(i.size())"
      ],
      "metadata": {
        "id": "D_cxbEdgSJO3"
      },
      "id": "D_cxbEdgSJO3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79737168-7660-425c-8a04-70829df2ea9b",
      "metadata": {
        "id": "79737168-7660-425c-8a04-70829df2ea9b"
      },
      "outputs": [],
      "source": [
        "# test policy\n",
        "env = gym.make(\"MountainCarContinuous-v0\")\n",
        "model = CompoundModel(pModel, vModel)\n",
        "policy = PolicyModel(2,1,20)\n",
        "stateDict = policy.state_dict()\n",
        "#params = sample_weights(2,20,1)\n",
        "params = params_from_ng('policy.npy')\n",
        "for weight, p in zip(policy.state_dict(), params):\n",
        "  stateDict[f\"{weight}\"] = p\n",
        "episodes = 100\n",
        "state, _ = env.reset()\n",
        "count = 0\n",
        "for episode in range(episodes):\n",
        "  for _ in range(100):\n",
        "    action = policy(torch.tensor(state, dtype= torch.float32))\n",
        "    with torch.no_grad():\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "      if terminated or truncated:\n",
        "        print(terminated, truncated)\n",
        "        count+=1\n",
        "        break\n",
        "    state = next_state\n",
        "print(count/100)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da41c0a-2a8a-4e7e-a5c4-7f076e3e6885",
      "metadata": {
        "id": "7da41c0a-2a8a-4e7e-a5c4-7f076e3e6885"
      },
      "source": [
        "## visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf77243b-7922-48d9-b425-2bd6d823ffa0",
      "metadata": {
        "id": "cf77243b-7922-48d9-b425-2bd6d823ffa0"
      },
      "source": [
        "Since visualization in jupyter can be tricky, we write a small python script for it. We simply save the weights here and reload them in the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f79198-8b55-4a45-8c4f-0ecbddc7bb80",
      "metadata": {
        "id": "a6f79198-8b55-4a45-8c4f-0ecbddc7bb80"
      },
      "outputs": [],
      "source": [
        "#np.save(\"policy.npy\", recommendation.value)\n",
        "env = gym.make(\"MountainCarContinuous-v0\",render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypeVarTuple\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "params = params_from_ng('policy.npy')\n",
        "for weight, p in zip(policy.state_dict(), params):\n",
        "  stateDict[f\"{weight}\"] = p\n",
        "state, _ = env.reset()\n",
        "a = env.render()\n",
        "img = plt.imshow(a)\n",
        "episode_length = 200\n",
        "for _ in range(episode_length):\n",
        "    img.set_data(env.render())\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    action = policy(torch.tensor(state, dtype= torch.float32))\n",
        "    with torch.no_grad():\n",
        "      next_state, _, terminated, truncated, _ = env.step(action)\n",
        "      if terminated or truncated:\n",
        "        print('Terminated')\n",
        "        break\n",
        "    state = next_state"
      ],
      "metadata": {
        "id": "TOIHZTxO6UCo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "d1a527d3-dc2f-4420-dc66-22a204aa2c35"
      },
      "id": "TOIHZTxO6UCo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAF7CAYAAAD4/3BBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNq0lEQVR4nO3deVhU9eI/8PfMAMMmIAoMCCgqiqC4oOK4YpCo4JaWu6Ze/WZYll01yizvrWvLrdRyqdtNLfWqWZqSG6JCJi6o5BakpkKxKjLDPsPM5/eH1/ldUkt04MzA+/U853lkzpmZ95zQeXfO53yOTAghQERERGRB5FIHICIiIvo9FhQiIiKyOCwoREREZHFYUIiIiMjisKAQERGRxWFBISIiIovDgkJEREQWhwWFiIiILA4LChEREVkcFhQiIiKyOJIWlJUrV6JVq1awt7dHeHg4Tpw4IWUcIiIishCSFZQtW7Zg3rx5eP3113H69Gl07twZ0dHRKCgokCoSERERWQiZVDcLDA8PR48ePfDxxx8DAIxGI/z8/PDcc8/h5ZdfliISERERWQgbKd5Up9Ph1KlTiI+PNz0ml8sRFRWF1NTUu7avqqpCVVWV6Wej0YiioiI0a9YMMpmsXjITERHRoxFCoKSkBD4+PpDL//gkjiQF5caNGzAYDPDy8qrxuJeXFzIyMu7afunSpViyZEl9xSMiIqI6lJ2dDV9f3z/cRpKCUlvx8fGYN2+e6WeNRgN/f39kZ2fDxcVFwmRERET0oLRaLfz8/NCkSZM/3VaSgtK8eXMoFArk5+fXeDw/Px8qlequ7ZVKJZRK5V2Pu7i4sKAQERFZmQcZniHJVTx2dnYICwtDUlKS6TGj0YikpCSo1WopIhEREZEFkewUz7x58zB16lR0794dPXv2xLJly1BWVoZp06ZJFYmIiIgshGQFZezYsSgsLMTixYuRl5eHLl26YO/evXcNnCUiIqLGR7J5UB6FVquFq6srNBoNx6AQERFZidp8f/NePERERGRxWFCIiIjI4rCgEBERkcVhQSEiIiKLw4JCREREFocFhYiIiCwOCwoRERFZHBYUIiIisjhWcTdjIiIi+mOVlZdRUpIEW1tf2Nm1gJ2dH2xsmkkd66GxoBARETUAlZUXkJ39ImxsPGBr6wkbGw/Y2bWAUtkB9vZBcHAIgp1dwAPdSdgSsKAQERE1EEJUQK/Pgl6f9d9H5JDLHU2LQtEUjo7d4egYBienMDg6dsHt0R7/v7RYSoFhQSEiImqwjDAaS2E0lv7352uoqEjHzZufA5BBJrODk1N3ODp2h5NTTzg6hkGhaAKZTAm5XAmZTAmZTJrhqiwoREREjYoAYLj9J1GN0tIUlJammNYqlUFwcOj43yUErq4xkMsd6j0lCwoRERGZGI0l0Ot/g0ymgNFYiiZNolhQiIiIqD7JYW8fDHv7EDg4BMPBIRg2Np6wsWlmWmQyW0mSsaAQERE1aDLTIpc7wtGxB5ycevx37Ek3yOUOkMnsIZfbQyaz5yBZIiIiMjcF5HIH02Jj4wFHxx7/vWqnBxwcQvC/hQWwnKt2fo8FhYiIqAEoLAQKCtTo1m007O07wMEhGHZ2flLHemgsKERERA3AxYvAjz8+jiFDXpA6ilnwXjxERERkcVhQiIiIyOKwoBAREZHFYUEhIiIii8OCQkRERBaHBYWIiIgsDgsKERERWRwWFCIiIrI4LChERERkcVhQiIiIyOKwoBAREZHFMXtBeeONNyCTyWosQUFBpvWVlZWIi4tDs2bN4OzsjNGjRyM/P9/cMYiIiMiK1ckRlJCQEOTm5pqWI0eOmNa9+OKL2LVrF7766iskJycjJycHTzzxRF3EICIiIitVJ3cztrGxgUqluutxjUaDf//739i0aRMee+wxAMDatWvRoUMHHDt2DL169aqLOERERGRl6uQIyqVLl+Dj44PWrVtj4sSJyMrKAgCcOnUKer0eUVFRpm2DgoLg7++P1NTU+75eVVUVtFptjYWIiIgaLrMXlPDwcKxbtw579+7F6tWrcfXqVfTr1w8lJSXIy8uDnZ0d3NzcajzHy8sLeXl5933NpUuXwtXV1bT4+fmZOzYRERFZELOf4hkyZIjpz6GhoQgPD0fLli2xdetWODg4PNRrxsfHY968eaaftVotSwoREVEDVueXGbu5uaFdu3a4fPkyVCoVdDodiouLa2yTn59/zzErdyiVSri4uNRYiIiIqOGq84JSWlqKK1euwNvbG2FhYbC1tUVSUpJpfWZmJrKysqBWq+s6ChEREVkJs5/i+etf/4phw4ahZcuWyMnJweuvvw6FQoHx48fD1dUVM2bMwLx58+Du7g4XFxc899xzUKvVvIKHiIiITMxeUH799VeMHz8eN2/ehIeHB/r27Ytjx47Bw8MDAPDhhx9CLpdj9OjRqKqqQnR0NFatWmXuGERERGTFzF5QNm/e/Ifr7e3tsXLlSqxcudLcb01EREQNBO/FQ0RERBaHBYWIiIgsDgsKERERWRwWFCIiIrI4dXKzQCIiIrIeQgjo9XpUVlZCoVBACAEhBAwGA3Q6HRwdHaFQKKBQKCCXy6FQKAAAMpmszjKxoBARETUS1dXVuHnzJnJzc033wCsvL0dZWRk0Gg1+++03qFQqCCFgNBpRWlqK7OxsBAcHw9bWFjY2NpDJZFAqlXBzczPN7u7s7AxbW1u0atXKVF4eFQsKERFRA1VWVobTp08jPT0dWq0W+fn5UCqV0Ol0KCkpQWBgIOzs7GBnZwelUom2bdvCxcXFdLRECIE2bdrAwcEBOp0OVVVVqKyshFarxY0bN6DX66HT6UyzxgcEBKBly5Zo06aN6c9OTk4PlZ0FhYiIqAE5deoUzp8/j2PHjuHatWtwd3eHp6cn1Go1Bg4cCGdnZzg6OsLOzg4uLi5QKpWws7Or1ZEPIQQqKipMS3l5OYqLi1FRUYFr164hLS0NmzdvxrVr19CnTx9ERESgd+/etfocLChERERW6M64Eb1ej5s3b+LIkSP497//jV27dmHIkCF48skn0alTJzg4OEChUMDW1hYKhcIs40ZkMhkcHR3h6Oh4V6bevXujuroa1dXVqKiowJEjR7Br1y689tprCAgIePD3EEKIR05az7RaLVxdXaHRaHhnYyIialSMRiNu3LiBa9eu4eDBg7h06RKysrLQunVrhIWFYcaMGZDL//9FunU5kPVB3KkZOp0O33//PR5//PEH+v7mERQiIiIrUFJSgitXruDHH3/Er7/+iqKiInh4eOCZZ55B9+7dJS8i93Mnl1KpRM+ePR/4eSwoREREFqy8vBz79+/H3r174e7ujpYtW6J///4ICQmBu7u71PHqDAsKERGRhblzWmTPnj3YsGED3N3dMWzYMHTu3Bmenp6ws7OTOGHdY0EhIiKyEHfmHtm7dy/+9a9/ISgoCK+++iratm0LW1vbGmNLGjoWFCIiIguQm5uLs2fPIiEhAdXV1Vi2bBk6dOjQqErJ/2JBISIiklBOTg6+//57XLp0CdXV1Zg6dSq6du1qthlZrRULChERkQR0Oh3279+PHTt2IDg4GFFRUejatSuUSqXU0SwCCwoREVE9EkKgrKwMS5YsQUlJCSZPnoywsDA4OTlZ7KXCUmBBISIiqgfV1dXQaDQ4cOAAli9fjpdffhlDhgwx3YCPamJBISIiqmN6vR5JSUn46quv0Lp1a+zdu5czof8JFhQiIqI6dP36dWzZsgVVVVV46qmnEBERwXEmD4AFhYiIqA4IIbB3717s378f/fv3R+/eveHl5SV1LKvBgkJERGRGQghkZ2fjjTfegKurK+Li4hAQENDoLxuuLRYUIiIiM9Hr9fjll1/w/vvvo3PnzoiLi4NCoeAg2IfAgkJERGQGBQUFSE5OxsGDB/GXv/ylVnfupbuxoBARET2izMxMfPvtt3BycsLf//53NG/eXOpIVo8FhYiI6CEZjUYkJSVh8+bNmDBhAnr37g0HBwepYzUILChEREQPQa/XY8OGDTh+/DjeeecdNG3alANhzYgFhYiIqBaMRiPy8/Px73//G0qlEqtXrwYADoQ1MxYUIiKiB1RZWYkffvgBycnJ6NKlC4YNG8ZiUkdYUIiIiB6A0WjEt99+i7179+LZZ59F165dYWPDr9G6Iq/tE1JSUjBs2DD4+PhAJpNhx44dNdYLIbB48WJ4e3vDwcEBUVFRuHTpUo1tioqKMHHiRLi4uMDNzQ0zZsxAaWnpI30QIiKiuvTxxx/jl19+waJFi9CjRw+WkzpW64JSVlaGzp07Y+XKlfdc/+6772LFihVYs2YNjh8/DicnJ0RHR6OystK0zcSJE3HhwgUkJiYiISEBKSkpmDVr1sN/CiIiojoghEBFRQVee+01KJVKvPDCC2jTpo3UsRoFmRBCPPSTZTJs374dI0eOBHD7P6SPjw9eeukl/PWvfwUAaDQaeHl5Yd26dRg3bhx++uknBAcH4+TJk+jevTsAYO/evRg6dCh+/fVX+Pj4/On7arVauLq6QqPR8G6QRERUJwwGA37++Wds3LgRnTp1wqhRo2BnZyd1LKtWm+/vWh9B+SNXr15FXl4eoqKiTI+5uroiPDwcqampAIDU1FS4ubmZygkAREVFQS6X4/jx4/d83aqqKmi12hoLERFRXRFCIC0tDe+99x769euHMWPGsJzUM7MWlLy8PAC4626NXl5epnV5eXnw9PSssd7Gxgbu7u6mbX5v6dKlcHV1NS1+fn7mjE1ERFTD4cOHkZCQgClTpiA6Oprzm0jArAWlrsTHx0Oj0ZiW7OxsqSMREVEDJITA9u3bcfjwYcycORMRERFSR2q0zDoEWaVSAQDy8/Ph7e1tejw/Px9dunQxbVNQUFDjedXV1SgqKjI9//eUSiWUSqU5oxIREdWg1+uxa9cuZGZmYs6cObyfjsTMegQlICAAKpUKSUlJpse0Wi2OHz8OtVoNAFCr1SguLsapU6dM2xw8eBBGoxHh4eHmjENERPSnhBDQ6XTYtm0bLl++jL/85S/w8PDgBGwSq/URlNLSUly+fNn089WrV5Geng53d3f4+/vjhRdewJtvvonAwEAEBATgtddeg4+Pj+lKnw4dOmDw4MGYOXMm1qxZA71ejzlz5mDcuHEPdAUPERGRua1atQparRbPP/883NzcpI5DeIiCkpaWhoEDB5p+njdvHgBg6tSpWLduHRYsWICysjLMmjULxcXF6Nu3L/bu3Qt7e3vTczZu3Ig5c+YgMjIScrkco0ePxooVK8zwcYiIiB5cVVUVXnvtNXTt2hV/+ctf4OzsLHUk+q9HmgdFKpwHhYiIHoUQAuXl5XjrrbfQv39/REVFcWbYelCb72/+1yAiokZFCIFbt25h7dq16NGjBwYNGgS53Couam1UWFCIiKhRKSgowOeffw5fX1+MGjVK6jh0H6yMRETUaBQUFGDNmjVQqVSYPHmy1HHoD/AIChERNQr5+flYtWoVBgwYUONiD7JMLChERNSgCSFw8+ZN/Otf/0JkZCT69u3LOU6sAAsKERE1WHfKyaZNm9ClSxf069eP5cRKsKAQEVGDde3aNWzevBmtW7dGbGys1HGoFjhIloiIGqSioiIsW7YMLVq0wNixY6WOQ7XEIyhERNTglJSU4J///CeGDx+Oxx57TOo49BBYUIiIqMEQQqCyshIrV65E3759ERERwTEnVooFhYiIGgydToeNGzeiefPmGDJkCMuJFeMYFCIiahCMRiPWr1+P4uJiTJ8+neXEyvEIChERNQgffPABZDIZnnvuOd5bpwFgQSEiIqu3evVqNGnSBJMnT4ZSqZQ6DpkBCwoREVktg8GAnTt3wmAwYMKECXBwcJA6EpkJj4EREZFVMhgM+OGHH3DlyhU88cQTcHV15biTBoQFhYiIrI4QAmlpaThy5AiGDx8OHx8fqSORmbGgEBGR1UlISMCyZcswatQotGvXTuo4VAc4BoWIiKyGEALXr1/Htm3bsGjRInTo0EHqSFRHeASFiIisghAChYWFWLFiBRYsWIDg4GCpI1Ed4hEUIiKyCiUlJVi/fj0iIyMREhIidRyqYzyCQkREFk+n02HTpk3w9PREVFSU1HGoHvAIChERWbw1a9ZALpdj9OjRnIitkWBBISIiiyWEwFtvvYWMjAysWbMGzs7OUkeiesKCQkREFslgMCAlJQUVFRVYvXo1y0kjwzEoRERkcYxGIy5cuICUlBTMnDkTTZo0kToS1TMWFCIisjgFBQX4+uuvMWTIELRq1UrqOCQBFhQiIrIoOp0Oy5YtQ+/evREWFiZ1HJIICwoREVkMg8GAJUuWoEuXLnjsscegUCikjkQSYUEhIiKLUFVVhVdeeQW5ubl46qmnYGtrK3UkklCtC0pKSgqGDRsGHx8fyGQy7Nixo8b6p59+GjKZrMYyePDgGtsUFRVh4sSJcHFxgZubG2bMmIHS0tJH+iBERGS99Ho9Dhw4AHd3d3z00UeQy/n/z41drX8DysrK0LlzZ6xcufK+2wwePBi5ubmm5T//+U+N9RMnTsSFCxeQmJiIhIQEpKSkYNasWbVPT0REDcK5c+eQlpaGiRMnwsnJSeo4ZAFqPQ/KkCFDMGTIkD/cRqlUQqVS3XPdTz/9hL179+LkyZPo3r07AOCjjz7C0KFD8c9//hM+Pj61jURERFasoKAAW7Zswbhx49CiRQup45CFqJNjaIcPH4anpyfat2+P2bNn4+bNm6Z1qampcHNzM5UTAIiKioJcLsfx48fv+XpVVVXQarU1FgDYvHkzjEZjXXwEIiKqB1VVVXjvvfcQGRmJ0NBQyGQyqSORhTB7QRk8eDC++OILJCUl4Z133kFycjKGDBkCg8EAAMjLy4Onp2eN59jY2MDd3R15eXn3fM2lS5fC1dXVtPj5+QEArl+/jiNHjphem4iIrIdGo8H777+P4OBgPP7447xih2owe0EZN24chg8fjk6dOmHkyJFISEjAyZMncfjw4Yd+zfj4eGg0GtOSnZ0NABg5ciQOHTqEX375BUIIM30CIiKqa1VVVfjss89QXFyMqVOn8sgJ3aXOh0m3bt0azZs3x+XLlwEAKpUKBQUFNbaprq5GUVHRfcetKJVKuLi41FgAoH379ujTpw82bNjAq4CIiKzIwYMHUVFRgVdeeYVX7NA91flvxa+//oqbN2/C29sbAKBWq1FcXIxTp06Ztjl48CCMRiPCw8Nr/foDBgxAcHAwli1bxqMoRERWICMjAydOnMBTTz0FV1dXqeOQhap1QSktLUV6ejrS09MBAFevXkV6ejqysrJQWlqK+fPn49ixY7h27RqSkpIwYsQItG3bFtHR0QCADh06YPDgwZg5cyZOnDiBH374AXPmzMG4ceMe6goeW1tbjB49GjqdDh9++CH0en2tX4OIiOqeEAI3b97E1q1bERERgbZt2/LUDt1XrQtKWloaunbtiq5duwIA5s2bh65du2Lx4sVQKBQ4e/Yshg8fjnbt2mHGjBkICwvD999/D6VSaXqNjRs3IigoCJGRkRg6dCj69u2LTz/99KE/hI2NDRYtWoRTp05h27ZtvLKHiMgCVVVVYf369fD390f//v15aof+kExY4XkRrVYLV1dXaDQa03gU4PbppFWrVmHcuHEIDQ2VMCEREf3emjVrUFxcjJdfflnqKCSR+31/30uDqq/e3t6IiYnB7t27kZubK3UcIiL6rw0bNiA9PR3PPfec1FHISjSogqJQKNCjRw+0bNkSX3/9NaqqqqSORETUqAkhcPr0aVy6dAkvvvgiHB0dpY5EVqJBFRQAsLOzw1NPPYW8vDwkJCRwPAoRkUSEEMjLy8O+ffswaNAgtGvXjoNi6YE1uIIC3D6S8uabb+I///kPjh49KnUcIqJGSa/XY/v27VCpVOjTpw/LCdVKgywod/zjH//AZ599htOnT0sdhYioURFCYOfOnSgsLMT48eOljkNWqEEXlDZt2uDpp5/G7t278euvv0odh4io0Th48CBOnTqFuLg42NvbSx2HrFCDLigKhQJ9+vRBQEAA9uzZg4qKCqkjERE1aEIInDx5EitXrsSzzz6L5s2bSx2JrFSDLijA7Zlmx40bh4sXLyItLY3T4RMR1aGioiKsW7cOr776Knx9faWOQ1aswRcU4PaRlPnz52PLli24cOGC1HGIiBqk8vJy7NixA71790bHjh05KJYeSaMoKADg4+ODmTNn4vPPP8e1a9ekjkNE1KAYDAZ8//33uHXrFqKjo2vc3oToYTSaggIAoaGhGDFiBJYsWYLCwkKp4xARNRjXrl3D119/jbFjx3LcCZlFoyooMpkMarUa/fv3x8aNGzmJGxGRGVRXV2PWrFl49dVX4efnJ3UcaiAaVUEBbg+ajY2NhV6vR3JyMgwGg9SRiIislkajwYIFC7BgwQL4+/tLHYcakEZXUGQyGTw8PBAdHY3k5GRcu3aNV/YQET2E8vJyfP7557C3t0e/fv04KJbMqtEVlDtCQ0PRt29fLF++nKd6iIhqSQiBtLQ0FBcX4/nnn+dNAMnsGm1BAYCBAwciNDQU7733ntRRiIisSkFBAXbv3o1Ro0ZBpVJJHYcaoEZdUBQKBSZPnozKykps3bqV41GIiB6ATqfD6tWr0atXL3Tq1EnqONRANeqCAgB2dnb4v//7P2zcuBFJSUkcj0JE9AcMBgM2bNgApVKJESNGQKFQSB2JGqhGX1BkMhm8vb3xyiuv4OjRoygoKJA6EhGRxTp8+DDS0tKwcOFCDoqlOtXoC8odnTt3Rrt27bBz507eVJCI6B6OHDmCL7/8EvPmzWM5oTrHgvJf9vb2iI2NRXZ2No4cOcJTPURE/yM3Nxd79+7F6NGj0apVKxYUqnM2UgewJC4uLnjllVcQHR2Nzp07w9PTU+pIRESS0+v1OHToEDw9PTFkyBDY2PCrg+oej6D8jr29PT799FMsWLCA41GIqNETQuDMmTM4fvw4pk2bxnJC9YYF5R4CAwMRExODf/7zn8jJyZE6DhGRZK5cuYKNGzdi9uzZaNKkidRxqBFhQbkHuVyO6OhoODs7IzExkfOjEFGjpNFo8P7772PChAkICgqSOg41Miwo9+Hi4oJp06YhMzMTGRkZHDRLRI2KEALLli1DZGQkevToIXUcaoRYUP6Ar68vYmJisHHjRty6dYslhYgaBYPBgC+++AK5ubkYMGAAr9ghSbCg/AGZTIY+ffogMDAQn332Gaqrq6WORERUp4QQyMjIQEZGBp577jl4eHiwoJAkWFAewLRp01BUVITt27dLHYWIqE5VVFRg27Zt6N+/P0JCQqSOQ40YC8oDeumll5Ceno4jR45IHYWIqE4IIbB27Vp4eHggMjJS6jjUyNWqoCxduhQ9evRAkyZN4OnpiZEjRyIzM7PGNpWVlYiLi0OzZs3g7OyM0aNHIz8/v8Y2WVlZiImJgaOjIzw9PTF//nyLP33SvHlzDBs2DO+//z5+/PFHjkchogZFCIHExERcuXIFU6dOhZ2dndSRqJGrVUFJTk5GXFwcjh07hsTEROj1egwaNAhlZWWmbV588UXs2rULX331FZKTk5GTk4MnnnjCtN5gMCAmJgY6nQ5Hjx7F+vXrsW7dOixevNh8n6oOyGQy9OrVC2PHjkVycjIqKyuljkREZDa//PIL/v3vf+Nvf/sbnJycpI5DBJl4hEMBhYWF8PT0RHJyMvr37w+NRgMPDw9s2rQJY8aMAQBkZGSgQ4cOSE1NRa9evbBnzx7ExsYiJycHXl5eAIA1a9Zg4cKFKCwsfKDWrtVq4erqCo1GAxcXl4eN/1C0Wi0+/vhjhIWFISoqircaJyKrl5ubi3feeQdjx45FeHg45HKe/ae6UZvv70f6LdRoNAAAd3d3AMCpU6eg1+sRFRVl2iYoKAj+/v5ITU0FAKSmpqJTp06mcgIA0dHR0Gq1uHDhwj3fp6qqClqttsYiFRcXF4wbNw6JiYl3nd4iIrI2JSUl2LJlC3x9fRESEsJyQhbjoX8TjUYjXnjhBfTp0wcdO3YEAOTl5cHOzg5ubm41tvXy8kJeXp5pm/8tJ3fW31l3L0uXLoWrq6tp8fPze9jYZtG6dWtMmzYNixYtQlVVlaRZiIgeltFoxPnz55GTk4OpU6fW+xFpoj/y0AUlLi4O58+fx+bNm82Z557i4+Oh0WhMS3Z2dp2/558JDg7G1KlTsXjxYhiNRqnjEBHVWmlpKVasWIH/+7//g4eHh9RxiGp4qIIyZ84cJCQk4NChQ/D19TU9rlKpoNPpUFxcXGP7/Px8qFQq0za/v6rnzs93tvk9pVIJFxeXGovUZDIZBg0aBJVKhU2bNvFIChFZlbKyMixZsgTTpk1D69atpY5DdJdaFRQhBObMmYPt27fj4MGDCAgIqLE+LCwMtra2SEpKMj2WmZmJrKwsqNVqAIBarca5c+dQUFBg2iYxMREuLi4IDg5+lM9S7xwcHBAbG4uUlBScOnWKlx4TkVXQ6/VYt24dWrdujccff5wzxZJFsqnNxnFxcdi0aRO+/fZbNGnSxDRmxNXVFQ4ODnB1dcWMGTMwb948uLu7w8XFBc899xzUajV69eoFABg0aBCCg4MxefJkvPvuu8jLy8OiRYsQFxcHpVJp/k9YxwIDAzFu3DgcPHgQQUFBpgHDRESWKikpCQUFBVi4cCHLCVmsWh1BWb16NTQaDSIiIuDt7W1atmzZYtrmww8/RGxsLEaPHo3+/ftDpVLhm2++Ma1XKBRISEiAQqGAWq3GpEmTMGXKFPztb38z36eqZ3369IFKpcLWrVthMBikjkNEdF/p6enYtWsXnnjiCTg4OEgdh+i+HmkeFKlIOQ/K/VRVVWHu3LkYP348BgwYIHUcIqIahBAoKirCu+++i9DQUIwbN47zOFG9q7d5UOj/UyqV+Pjjj/H6668jJydH6jhERDUIIXD06FHY2tpiwoQJLCdk8VhQzEihUOCdd97BqlWr7junCxGRFE6fPo19+/Zh7ty5HHdCVoEFxYxkMhm6dOmC9u3b44svvjDNtEtEJKXr169j7dq1iIuL43wnZDVYUMxMqVQiJiYGN2/exJkzZ6SOQ0SNnMFgwD/+8Q9MmjQJQUFBUschemAsKHWgadOmmDp1Knbu3InffvuN86MQkSSqq6uxfv169OjRA127duWpHbIqLCh1QCaTITg4GBEREfjkk09QXl4udSQiamQMBgOSk5Nx4cIFREZGwt7eXupIRLXCglKHhg8fDjc3N3z++edSRyGiRqaoqAhbt25FbGzsXbN+E1kDFpQ69swzzyAnJwf79u2TOgoRNRJGoxFr166FWq1GRESE1HGIHgoLSh1zcHDAjBkz8P333yMzM5PjUYioTgkhsG3bNlRUVGDs2LEcd0JWiwWljslkMrRp0wa9evXC+vXrcevWLakjEVED9uOPP2LXrl1YuHAhp7Inq8aCUg9kMhkGDhwIBwcHHDx4ENXV1VJHIqIGqKCgAJ9++ilee+01q7z5KtH/YkGpJ05OTpg2bRrOnDmDM2fO8FQPEZlVcXExtm7disceewwtW7bkqR2yeiwo9cjX1xeTJ0/Ghx9+yFlmichs9Ho9EhISkJubi8jISB49oQaBBaWeBQUFYebMmViwYAGMRqPUcYjIygkhcOvWLezatQuzZ89G06ZNpY5EZBYsKBLo27cvunXrhnXr1kGv10sdh4isWFlZGV5//XXMnTsXLVq0kDoOkdmwoEjA1tYWo0aNQk5ODo4fP84jKUT0UCorK7FmzRr07NkTvXv35rgTalBYUCTi5eWFiIgIfP311/jtt9+kjkNEVighIQGVlZWYNGmS1FGIzI4FRUJhYWHo3Lkztm3bBp1OJ3UcIrIiZ86cwcWLFzFp0iTY2NhIHYfI7FhQJOTg4IBx48ahsLAQ+/bt46XHRPSnhBAoKChAYmIi+vbtC39/f57aoQaJBUVi9vb2ePPNN/HRRx/hypUrUschIgun1+vx5ZdfoqKiAgMHDoRczn/GqWHib7YFkMlkWL58OVasWIFff/1V6jhEZMFOnjyJrKwszJs3j0dOqEFjQbEAMpkMgYGBiIqKwjfffIPi4mKpIxGRBfrpp5+wefNmPP/882jSpInUcYjqFAuKhbCxsUFkZCSqq6tx6NAhXnpMRDWUlJTggw8+wPTp09GmTRup4xDVORYUC+Lk5ITo6Ghs2LABP/30EwfNEhEAwGAwYPXq1YiOjkanTp2kjkNUL1hQLExwcDBeeOEFrF+/Hrdu3ZI6DhFJrLq6GklJSbCxscHAgQOhUCikjkRUL1hQLIxMJkO/fv0QGhqKjz/+GNXV1VJHIiKJCCGQkZGBw4cPIzo6Gs2aNePAWGo0WFAs1MSJEwEAmzdvljgJEUlFr9fjk08+Qc+ePRESEiJ1HKJ6xYJiwWbNmoXLly/j6NGjHI9C1MgIIbBq1SoEBwdj6NChUschqncsKBZKJpPBy8sLw4YNw4EDB/Dbb7+xpBA1EkajEfv27UN2djZmzJgBOzs7qSMR1TsWFAsmk8kQFhYGHx8ffPXVV6ioqJA6EhHVg8zMTOzYsQOvvvoqywk1WiwoViA2NhZZWVk4cOAAj6IQNXD5+fnYvn07Jk2aBDc3N6njEEmmVgVl6dKl6NGjB5o0aQJPT0+MHDkSmZmZNbaJiIiATCarsTzzzDM1tsnKykJMTAwcHR3h6emJ+fPn82qVP6BSqTBv3jwkJSXh7NmzUschojpSUVGBvXv3wtfXF926deN9dqhRq9Vvf3JyMuLi4nDs2DEkJiZCr9dj0KBBKCsrq7HdzJkzkZuba1reffdd0zqDwYCYmBjodDocPXoU69evx7p167B48WLzfKIGys/PDy+99BIWLVqEkpISqeMQkZkJIXDw4EGkpKRg5MiRcHR0lDoSkaRk4hHOGRQWFsLT0xPJycno378/gNtHULp06YJly5bd8zl79uxBbGwscnJy4OXlBQBYs2YNFi5ciMLCwgc636rVauHq6gqNRgMXF5eHjW91hBA4fPgwdu/ejTfffBNKpVLqSERkJkVFRRg7diw2bNhg+reRqKGpzff3Ix0/1Gg0AAB3d/caj2/cuBHNmzdHx44dER8fj/LyctO61NRUdOrUqcZfwOjoaGi1Wly4cOGe71NVVQWtVltjaYxkMhl69uyJwMBA7NixA1VVVVJHIiIzKCwsxJIlS/D3v/+d5YTovx66oBiNRrzwwgvo06cPOnbsaHp8woQJ2LBhAw4dOoT4+Hh8+eWXmDRpkml9Xl7eXX8B7/ycl5d3z/daunQpXF1dTYufn9/DxrZ6Tk5OGDp0KC5duoQzZ87wpoJEVq60tBRffvklevfujZ49e0odh8hi2DzsE+Pi4nD+/HkcOXKkxuOzZs0y/blTp07w9vZGZGQkrly58tB34IyPj8e8efNMP2u12kZdUnx9fTFw4ECsXr0a7dq1u+sIFhFZj4SEBNja2mLYsGEcFEv0Px7qb8OcOXOQkJCAQ4cOwdfX9w+3DQ8PBwBcvnwZwO0rUvLz82tsc+dnlUp1z9dQKpVwcXGpsTR2PXv2xPDhw/Hee+/xKAqRFRJC4Ny5c8jIyMCwYcPg4OAgdSQii1KrgiKEwJw5c7B9+3YcPHgQAQEBf/qc9PR0AIC3tzcAQK1W49y5cygoKDBtk5iYCBcXFwQHB9cmTqNma2uLUaNGwd3dHWvXruVl2kRWRAiB3NxcbN26FZGRkWjZsiVvAkj0O7UqKHFxcdiwYQM2bdqEJk2aIC8vD3l5eaYZTq9cuYK///3vOHXqFK5du4adO3diypQp6N+/P0JDQwEAgwYNQnBwMCZPnowff/wR+/btw6JFixAXF8erUmpJLpdj7ty5yMjIwMGDB6WOQ0QPyGg04q233oKzszP69evHckJ0D7W6zPh+f4nWrl2Lp59+GtnZ2Zg0aRLOnz+PsrIy+Pn5YdSoUVi0aFGN0zLXr1/H7NmzcfjwYTg5OWHq1Kl4++23YWPzYENiGutlxvcihMD169fx6aefYtKkSTwKRWQFNm7ciJ9++glvvvmm1FGI6lVtvr8faR4UqbCg1FRdXY2UlBScOXMGU6ZMgYeHh9SRiOg+Dhw4gOTkZLz44osc4E6NTr3Ng0KWwcbGBn379oVcLsfXX38NnU4ndSQi+h0hBC5duoRDhw5h8uTJaNq0qdSRiCwaC0oDYWdnhxdeeAGHDx/G8ePHeVNBIguj0Wjw9ddfY8CAAQgMDOS4E6I/wYLSgMhkMqxYsQLr1q3DxYsXpY5DRP+l1+vx3XffwdHREQMGDGA5IXoALCgNjKenJ5577jls3LgRv/zyi9RxiBo9IQQ2btyIw4cPY+zYsbxakegBsaA0QJ06dUJERAQ2b97caO9bRGQpLl26hO+++w7z58/nfXaIaoEFpQFSKBSIiIiAq6srdu3axZlmiSRSVlaGefPmYdmyZWjXrp3UcYisCgtKA2Vra4snn3wSFy5cwJEjR1hSiOpZaWkpPvjgA8ydO/e+t/EgovtjQWmgZDIZPD09ERsbi08++QQXLlyQOhJRo1FZWYk9e/bAx8cHvXr1gkKhkDoSkdVhQWngevfujenTp+ODDz6ARqOROg5Rg2c0GpGeno6MjAwMGTIETZo0kToSkVViQWkEHnvsMYwdOxavv/4650chqmNVVVV48803MWXKFPj4+Egdh8hqsaA0EhEREejYsSPWrVvHOx8T1ZHKykqMHj0azz//PPz9/aWOQ2TVWFAaAZlMBnt7e8TExODGjRtITU2FwWCQOhZRg6LVarFs2TJMnz4djz/+OCdjI3pELCiNiLe3NwYPHowDBw7g+vXrPN1DZCaVlZXYtWsXmjZtitjYWJYTIjNgQWlkOnXqhD59+uCNN97gURQiMxBC4Pjx47h27RrGjBkDe3t7qSMRNQgsKI3QwIEDMXToULzyyis8ikL0CIQQyMnJwY4dO/Dkk0+iWbNmUkciajBYUBohW1tbjBkzBq1atcJnn30GvV4vdSQiq3Tr1i0sXboUTzzxBAIDA6WOQ9SgsKA0UjY2Npg4cSKKiopw6NAhlhSiWtJqtZg/fz48PDzQr18/jjshMjMWlEbM1dUVTz31FFJTU5GZmcnTPUQPSKfT4csvv0RYWBhee+01qeMQNUgsKI1cQEAABg8ejC+++IIzzRI9oF27dkGn02Hq1KmQy/nPKFFd4N8sQvfu3RESEoI5c+bwyh6iPyCEwOnTp3HhwgWMGTMGjo6OUkciarBYUAgKhQKTJ09G+/bt8cYbb6CyslLqSEQWRwiB3377Ddu3b0dsbCx8fX057oSoDrGgEABALpcjPj4eTZo0wbfffouqqiqpIxFZlPz8fKxcuRK9evVCt27dWE6I6hgLCpnY2Nhg5syZuHr1KlJSUjholui/Kioq8M4776Bt27aIiYmROg5Ro8CCQjU0bdoUTz31FJKTk/Hzzz9LHYfIInz88ccIDQ3F008/LXUUokaDBYXu0qpVK4wcORKrV6/GrVu3pI5DJBmj0YhvvvkG9vb2GD16NK/YIapH/NtGd5HL5QgLC0N4eDjmz5+PmzdvSh2JqN4ZjUakpaUhIyMDo0aNQpMmTTjuhKgesaDQPclkMowfPx7t2rXDsmXLOEcKNSpCCFy+fBl79uzB0KFDecUOkQRYUOgPvfjiiwgICMDWrVt5+TE1GllZWXj//fcxaNAgdOnSReo4RI0SCwr9IVtbW4wdOxZlZWXYs2cPr+yhBq+8vBwLFy7E1KlToVarpY5D1GixoNCfcnJywuTJk/HDDz/g7NmzLCnUYOn1erz55puYMWMGevXqJXUcokatVgVl9erVCA0NhYuLC1xcXKBWq7Fnzx7T+srKSsTFxaFZs2ZwdnbG6NGjkZ+fX+M1srKyEBMTA0dHR3h6emL+/Pmorq42z6ehOuPu7o45c+Zg1apVyMjIkDoOkdlVVlZi06ZNaN++Pe9OTGQBalVQfH198fbbb+PUqVNIS0vDY489hhEjRuDChQsAbo9X2LVrF7766iskJycjJycHTzzxhOn5BoMBMTEx0Ol0OHr0KNavX49169Zh8eLF5v1UZHYymQytWrXCpEmT8I9//AOnT5+WOhKR2VRXV2P//v3QaDSIjY2Fvb09CwqRxGTiEY/Xu7u747333sOYMWPg4eGBTZs2YcyYMQCAjIwMdOjQAampqejVqxf27NmD2NhY5OTkwMvLCwCwZs0aLFy4EIWFhbCzs3ug99RqtXB1dYVGo4GLi8ujxKdaEkJg7969SElJwcyZM9G6dWupIxE9sv379yMtLQ1PP/00fHx8pI5D1GDV5vv7ocegGAwGbN68GWVlZVCr1Th16hT0ej2ioqJM2wQFBcHf3x+pqakAgNTUVHTq1MlUTgAgOjoaWq3WdBTmXqqqqqDVamssJA2ZTIZBgwZh0KBB2LZtGwoLCzkmhayWEALfffcdPvnkE8yYMYPlhMiC1LqgnDt3Ds7OzlAqlXjmmWewfft2BAcHIy8vD3Z2dnBzc6uxvZeXF/Ly8gAAeXl5NcrJnfV31t3P0qVL4erqalr8/PxqG5vMSKFQoH///mjZsiW+/vprlJWVsaSQ1TEajThz5gw2bdqElStXwtPTU+pIRPQ/al1Q2rdvj/T0dBw/fhyzZ8/G1KlTcfHixbrIZhIfHw+NRmNasrOz6/T96M8pFArT5cfbtm2D0WiUOhLRAxNC4OrVq9i+fTtee+01qFQqjjkhsjC1Lih2dnZo27YtwsLCsHTpUnTu3BnLly+HSqWCTqdDcXFxje3z8/OhUqkAACqV6q6reu78fGebe1EqlaYrh+4sZBleeukl/PTTT/jiiy+kjkL0wG7cuIENGzZg8ODBCAoKkjoOEd3DI8+DYjQaUVVVhbCwMNja2iIpKcm0LjMzE1lZWabJjtRqNc6dO4eCggLTNomJiXBxcUFwcPCjRiGJvPrqq8jOzsann34qdRSiP6XX6/H2229DrVajd+/eUschovuoVUGJj49HSkoKrl27hnPnziE+Ph6HDx/GxIkT4erqihkzZmDevHk4dOgQTp06hWnTpkGtVpsmPBo0aBCCg4MxefJk/Pjjj9i3bx8WLVqEuLg4KJXKOvmAVPeaNGmCuLg4VFZW4ptvvuF4FLJYQgjMmTMHQ4cORWRkJE/rEFmwWhWUgoICTJkyBe3bt0dkZCROnjyJffv24fHHHwcAfPjhh4iNjcXo0aPRv39/qFQqfPPNN6bnKxQKJCQkQKFQQK1WY9KkSZgyZQr+9re/mfdTUb2SyWRwd3fH2LFjcenSJXz//fcwGAxSxyKqoby8HNOnT0dgYCAGDhwIhUIhdSQi+gOPPA+KFDgPiuW6evWq6dx+9+7d+X+oZBE0Gg22bt0KNzc3jBgx4oHnXCIi86qXeVCI7iUgIABPPvkkdu7ciR9++EHqOEQoLy/Hzp074ejoiOjoaJYTIivBgkJmFxQUhPHjx2P16tXYtWuX1HGoETMajdi0aRP0ej1iYmJ4xJXIirCgUJ3o0KED4uPjkZaWxjsgkySMRiPWrVuH0tJSjB079q5JJInIstlIHYAaJplMhpCQENNU4kqlEoGBgZDL2Ymp7lVUVOCTTz7BhQsXsGrVKtja2kodiYhqid8WVGdkMhk6deqEAQMGYOfOnbh8+TKPpFCdKy8vx+7du1FRUYF3332X5YTISrGgUJ1Tq9Xo27cvtmzZghMnTkgdhxownU6HAwcOoKioCNOnT0fTpk2ljkRED4kFheqFWq3GsGHD8P7779eYbZjIXIQQ+Pbbb3H9+nWMGDHirhuTEpF1YUGhetO5c2e8+uqrOHToEDIyMni6h8ymuroaGzZswM8//4zp06fzzsREDQAHyVK9kclkCA0NhcFgwPbt2zFq1Ci0a9eOA2fpkZSVleHjjz9GcXExlixZwnlOiBoIfjNQvZLJZOjWrRsiIiLw7bff4ty5c1JHIitWWlqKb7/9FkajES+99BLLCVEDwiMoJAm1Wg17e3skJCQgJycHQ4YMqdf3P3/+PHbv3o1+/fqhe/fuvNLDCul0Onz11VcAgBkzZqB58+YSJyIic2JBIcl06dIFDg4OePfddyGEwNChQ+vlfSsqKrBt2zb885//hI+PD7y9vTF+/HgMHToU/v7+9ZKBHt2HH34IlUqFESNGcBI2ogaINwskSQkhkJmZiX/9618YMWIE+vbtW6djUoQQuH79Ojp06IDKykrT4/b29lAqlejXrx9mz56N3r17w9HRETY2NhwjY2EqKyvx5ptvIiQkBKNHj+ZpHSIrwpsFktWQyWRo3749nn/+eSQmJiI5ORlGo7FO3zMzM7NGOQFuf+lpNBokJCQgJiYGnp6eWLx4MVJSUpCVlYXKykpedWQBbt26hUWLFqFNmzYYM2YMywlRA8YjKGQxcnJy8Omnn6Jjx44YM2ZMnbyHwWDA8OHDsXv37gfavkmTJujZsycGDhyIsLAwhISEoEWLFjyqUs+EEMjLy8OGDRvQtGlTPPnkk3B1dZU6FhHVUm2+v1lQyKIUFxfjyy+/RGVlJZ599lk4OTmZ9fWzsrLQpk0bVFdX1/q5fn5+CAkJQVhYGCIiItC3b1/Y29ubNR/d2/Xr17F8+XIMHjwYERERPHJCZKVYUMhqCSFQXl6OLVu2IDs7G3PnzoWrqytkMplZXn/IkCHYu3fvI72GUqmEp6cnPDw88OSTT5rmc7nDXFnp9u/D+fPn8f7772PhwoUICgri/iWyYiwoZNWEEDAYDNi6dSuuX7+OGTNmwMPD45G/mKqrq9GhQwdcvnzZTEkBW1tb2NraIiwsDM888wwGDhwINzc3KJVKngZ6RNXV1di2bRu+++47vPPOO/D29mY5IbJyHCRLVk0mk8HGxgYTJkxAu3btsGrVKrOUip07d+LWrVtmSPj/6fV6lJeX4/vvv8fEiRPh6+uLF198EQkJCfj5559RWlrKwbUPoaqqCvv370dKSgoWLFgAHx8flhOiRoZHUMjipaSk4ODBg+jXrx8iIyMf6jUMBgOeeuopfPPNN2ZOd28ODg4IDQ1F37590aNHD3Tt2hWtW7eGjQ2nHvozOp0Oa9euRUVFBUaNGoWWLVtKHYmIzKQ239/815IsXr9+/dC0aVOsX78e+fn5eOKJJ2o9OPX69esoKCioo4R3q6iowPHjx3H8+HF4eHigffv26Ny5MyIjIxEVFYUmTZrUWxZrkp+fj6VLl6J79+548skn4e7uLnUkIpIIj6CQVTAajfjtt9+wdu1aNG/eHNOnT3/gkiKEwKpVq/DXv/71rvlP6pONjQ3c3d3RtGlTnDx5kiXlfwghcOTIEXz22WeYOXMmevbsySt1iBogHkGhBkcul8PX1xevvPIKVq9ejbfeegvz5s1D06ZN//S5Go0G58+fR2VlJeRyOVxdXdGjRw+0bdsWrq6u0Ov1+O2333D27FlcvnwZVVVVdfIZqqurUVBQgIqKCo5L+S8hBIxGI9LS0rBy5UrMnDkTffr04XgTIuIRFLJOO3fuREpKCiZMmICQkBAolcr7bnvgwAE8/vjjsLOzQ2hoKKKiou559MVgMCAjIwOHDx9GYWFhnWVfvnw5nn32WY5Hwe3ymJSUhNTUVEybNg3BwcFSRyKiOsQjKNTgxcbGQqVSYcOGDQgLC8OIESPg7Ox813YGgwH5+fkAbt+cMDIy8r5lRqFQmMrOli1boNfr6yR7t27doFAo6uS1rcnly5exc+dO6HQ6LFy4kHcjJqIaeJkxWSW5XI7u3bvjueeew82bN7FixYp7ji8pLi7G0qVLERQUhL59+/7hkZY72rZti1GjRtVFbPTt2xctWrRo9KcwkpKSsGbNGgQFBWHBggUsJ0R0FxYUslpyuRwBAQGYPn06QkJCMHz4cFy5cqXGzQb1ej1+/fVXdOzYsVanAwMDA2vMDmsugwYNgre3t9lf11rodDps3LgRH3zwAWJjYzFo0CBOaEdE98RTPGT1nJ2dMWzYMHTr1g1//etfMWzYMIwcORJOTk7w8vLCyZMnsXHjxlq9po2NDfr374+ff/75rnUKhQJKpRK2traQy+UwGo3Q6/WorKz8wzsx29nZwdfX94GO4jQ0RqMRhYWF+OSTT1BVVYWtW7fC0dGx0R9JIqL7Y0GhBkEul8PPzw8ffvgh3nvvPWRlZeGJJ55Au3btoFAoav1FKJPJ7vkcNzc3tG/fHh06dICXlxfs7e1RXl6OnJwcXLx4EZcvX0ZJSck9XzM8PBy9e/dudF/KRUVFOHHiBPbv348+ffpg9OjRUkciIivAgkINio+PD+Lj47Fv3z5s3rwZHTt2RLdu3czy2l5eXoiKikLr1q1rDHJ1cnJCYGAgAgICkJmZiQMHDtxzSn0/P79GNSuqEAIFBQVYtWoVysrKMH36dF6lQ0QPrFYnf1evXo3Q0FC4uLjAxcUFarUae/bsMa2PiIgw/Z/nneWZZ56p8RpZWVmIiYmBo6MjPD09MX/+fFRXV5vn0xAB8PT0xOTJkzFhwgRkZ2cjPj7eLK87bNgwBAYG3vcKHBsbGwQHB2PQoEH3XK9UKms9A641S0xMxNy5c9G6dWu89NJL6NixI8ebENEDq9URFF9fX7z99tsIDAyEEALr16/HiBEjcObMGYSEhAAAZs6cib/97W+m5zg6Opr+bDAYEBMTA5VKhaNHjyI3NxdTpkyBra0t/vGPf5jpIxHdFhgYiFmzZqFbt27Yvn37A03qdofRaMSvv/4KW1tbGAwGTJgwAS1atPjT58lkMrRv3x6xsbFISEgwPd6mTRvMmzfvoT6HNTEajdBqtVi2bBmKi4uxfPlyNG3alLPCElGt1ep/Z4YNG4ahQ4earnB466234OzsjGPHjpm2cXR0hEqlMi3/e+XE/v37cfHiRWzYsAFdunTBkCFD8Pe//x0rV66ETqcz36ciwu2y4OjoiP79+2PKlCm1GpzaokULfPPNN0hMTMTTTz8NLy+vBx47IpfL0bJlS/j7+5sec3FxMZX4hkgIAa1Wi127dmHSpElo3bo13n33XXh5ebGcENFDeejjrQaDAZs3b0ZZWRnUarXp8Y0bN6J58+bo2LEj4uPjUV5eblqXmpqKTp06wcvLy/RYdHQ0tFotLly4cN/3qqqqglarrbEQPYg7pxq7deuGXr16PdDsrSqVClOmTIG9vT0GDBiAESNG1Pq+OR4eHvD19QVwu7D07NmzwQ6ONRgMSEtLw7/+9S/s27cP8fHxmDJlCosJET2SWg+SPXfuHNRqNSorK+Hs7Izt27ebBr5NmDABLVu2hI+PD86ePYuFCxciMzPTdIv7vLy8GuUEgOnnvLy8+77n0qVLsWTJktpGJaqhe/fuMBqNOHbs2D1niZXJZGjZsiUiIiLM+uWqVCrxxhtvmO31LMnVq1exZcsW6HQ6dOnSBdOmTeMdiInILGpdUNq3b4/09HRoNBps27YNU6dORXJyMoKDgzFr1izTdp06dYK3tzciIyNx5coVtGnT5qFDxsfH1zh/r9Vq4efn99CvR42Ts7Mz+vTpg1atWuHHH3/EtWvXUFJSAp1OBw8PD/Ts2RNBQUFwdXU162BOmUx2VzG3dhUVFVi7di1OnDiB4cOHIzw8HD4+Pg32KBER1b9aFxQ7Ozu0bdsWABAWFoaTJ09i+fLl+OSTT+7aNjw8HMDte260adMGKpUKJ06cqLHNnfukqFSq+76nUqlslJNbkfkplUoEBATAz88PBoMBRqMR165dw1tvvYWcnBzMmjULzs7ONeZBedj75tyZf2XFihXm/AiSMRqN0Ol0OH78OD744AO0atUKCxYsQLt27XjjQyIyu0f+V8VoNN739vTp6ekAYJraW61W46233kJBQQE8PT0B3L4U0cXFhfMjUL2RyWSwtbWFra0tAKBDhw747LPP8PXXX2P27NmIiYlBdHQ0/P394ejoiC5duuDy5csoLi5+4Pdwc3PDk08+CQcHB4waNcqqjywYjUbcuHEDFy9exHfffYeysjK8/fbbCAoKAgCr/mxEZLlkQgjxoBvHx8djyJAh8Pf3R0lJCTZt2oR33nkH+/btQ+vWrbFp0yYMHToUzZo1w9mzZ/Hiiy/C19cXycnJAG4PpuvSpQt8fHzw7rvvIi8vD5MnT8Zf/vKXWl1mXJvbNRPVxrlz55CYmIj8/Hy0bNkSXbt2RadOnXD69GkkJyf/4VT2dyiVSjz++OMICwurh8R1KycnB8ePH8fRo0dRVlaG0aNHP/BNF4mIfq8239+1KigzZsxAUlIScnNz4erqitDQUCxcuBCPP/44srOzMWnSJJw/fx5lZWXw8/PDqFGjsGjRohohrl+/jtmzZ+Pw4cNwcnLC1KlT8fbbb9fqEDELCtW1rKwsHDt2DBcvXkRRURGio6NhNBpx+vTpP3yeTCbDgAEDMGDAgHpKWjdycnKwY8cOXLt2Dd7e3mjXrh26devWqG90SESPrs4KiqVgQaH6YDAYUFhYiKSkJKSlpeHkyZOIiIiAvb09qqur7zq1YWtri4iICISFhVnVEYb//Sfgl19+wUcffYSrV68iNjYWvXv3hr+/f60vsyYiuhcWFCIzMhgM0Ov1uHHjBj755BOcOXMGgYGBcHZ2hkKhQLNmzRAYGIiwsDC4u7tDLpdbxbgMIQQqKytRWVmJ9PR0fPnll8jOzsbgwYMxceJENGvWDDY2NlbxWYjIOrCgENWhnJwcfPzxxzh58iRCQ0PRo0cPBAUFoWnTpmjRooVFX9EihEB5eTny8vKQl5eHxMREnDhxAv7+/hg/fjz69Olj0fmJyLqxoBDVA71ejxMnTuDYsWMoKChAQUEBOnTogA4dOiAwMBAtW7aEg4OD1DEBADdv3sS5c+dw9epVFBYWorS01DRWbMCAAejatavUEYmoEWBBIapHdy7DPXPmjKmo5OXlobS0FA4ODoiIiEBoaCh8fX3r5eiEEAJ6vR7p6enIyMhAeno6ysvLUVZWhsDAQHTr1g1t2rRBQEAAlEolT+EQUb1hQSGSiBACJSUl0Gq1uHHjBrZs2YKysjL88ssv0Gg0CA4OhouLCx577DG0b98e3t7eNQbUPmhZ+N+/tjqdDhcvXsSlS5dw8eJFXLhwAZcuXYK/v7/p6Iifnx8cHBzg5ubGAa9EJBkWFCILIISAwWCAEAJGoxEajQY//vgjNm3aBJ1Oh9zcXNy8eRNNmzZFVVUVQkJC4OnpCWdnZzg7OyM/Px8qlQp2dnbQ6/XQ6/W4cOEC7OzsUFlZiYKCAuTn50Oj0cDPzw89evRASEgIQkJC0K5dO9jZ2UEul5sG7fJICRFJjQWFyErodDrk5+fj2LFjsLGxgcFgQFlZGUpLS5GZmQl3d3e4urrC1tYWNjY2yMvLQ0BAAFq3bg0PDw94eHjAzc3Naq4cIqLGrTbf3xyuTyQhOzs7+Pn58eaXRES/Y75bthIRERGZCQsKERERWRwWFCIiIrI4LChERERkcVhQiIiIyOKwoBAREZHFYUEhIiIii8OCQkRERBaHBYWIiIgsDgsKERERWRwWFCIiIrI4LChERERkcVhQiIiIyOKwoBAREZHFYUEhIiIii8OCQkRERBaHBYWIiIgsDgsKERERWRwWFCIiIrI4LChERERkcVhQiIiIyOKwoBAREZHFYUEhIiIii8OCQkRERBaHBYWIiIgsjo3UAR6GEAIAoNVqJU5CRERED+rO9/ad7/E/YpUFpaSkBADg5+cncRIiIiKqrZKSEri6uv7hNjLxIDXGwhiNRmRmZiI4OBjZ2dlwcXGROpLV0mq18PPz4340A+5L8+G+NA/uR/PhvjQPIQRKSkrg4+MDufyPR5lY5REUuVyOFi1aAABcXFz4y2IG3I/mw31pPtyX5sH9aD7cl4/uz46c3MFBskRERGRxWFCIiIjI4lhtQVEqlXj99dehVCqljmLVuB/Nh/vSfLgvzYP70Xy4L+ufVQ6SJSIioobNao+gEBERUcPFgkJEREQWhwWFiIiILA4LChEREVkcqywoK1euRKtWrWBvb4/w8HCcOHFC6kgWJyUlBcOGDYOPjw9kMhl27NhRY70QAosXL4a3tzccHBwQFRWFS5cu1dimqKgIEydOhIuLC9zc3DBjxgyUlpbW46eQ3tKlS9GjRw80adIEnp6eGDlyJDIzM2tsU1lZibi4ODRr1gzOzs4YPXo08vPza2yTlZWFmJgYODo6wtPTE/Pnz0d1dXV9fhRJrV69GqGhoaZJrtRqNfbs2WNaz3348N5++23IZDK88MILpse4Px/MG2+8AZlMVmMJCgoyred+lJiwMps3bxZ2dnbi888/FxcuXBAzZ84Ubm5uIj8/X+poFmX37t3i1VdfFd98840AILZv315j/dtvvy1cXV3Fjh07xI8//iiGDx8uAgICREVFhWmbwYMHi86dO4tjx46J77//XrRt21aMHz++nj+JtKKjo8XatWvF+fPnRXp6uhg6dKjw9/cXpaWlpm2eeeYZ4efnJ5KSkkRaWpro1auX6N27t2l9dXW16Nixo4iKihJnzpwRu3fvFs2bNxfx8fFSfCRJ7Ny5U3z33Xfi559/FpmZmeKVV14Rtra24vz580II7sOHdeLECdGqVSsRGhoq5s6da3qc+/PBvP766yIkJETk5uaalsLCQtN67kdpWV1B6dmzp4iLizP9bDAYhI+Pj1i6dKmEqSzb7wuK0WgUKpVKvPfee6bHiouLhVKpFP/5z3+EEEJcvHhRABAnT540bbNnzx4hk8nEb7/9Vm/ZLU1BQYEAIJKTk4UQt/ebra2t+Oqrr0zb/PTTTwKASE1NFULcLotyuVzk5eWZtlm9erVwcXERVVVV9fsBLEjTpk3FZ599xn34kEpKSkRgYKBITEwUAwYMMBUU7s8H9/rrr4vOnTvfcx33o/Ss6hSPTqfDqVOnEBUVZXpMLpcjKioKqampEiazLlevXkVeXl6N/ejq6orw8HDTfkxNTYWbmxu6d+9u2iYqKgpyuRzHjx+v98yWQqPRAADc3d0BAKdOnYJer6+xL4OCguDv719jX3bq1AleXl6mbaKjo6HVanHhwoV6TG8ZDAYDNm/ejLKyMqjVau7DhxQXF4eYmJga+w3g72RtXbp0CT4+PmjdujUmTpyIrKwsANyPlsCqbhZ448YNGAyGGr8MAODl5YWMjAyJUlmfvLw8ALjnfryzLi8vD56enjXW29jYwN3d3bRNY2M0GvHCCy+gT58+6NixI4Db+8nOzg5ubm41tv39vrzXvr6zrrE4d+4c1Go1Kisr4ezsjO3btyM4OBjp6ench7W0efNmnD59GidPnrxrHX8nH1x4eDjWrVuH9u3bIzc3F0uWLEG/fv1w/vx57kcLYFUFhUhKcXFxOH/+PI4cOSJ1FKvUvn17pKenQ6PRYNu2bZg6dSqSk5OljmV1srOzMXfuXCQmJsLe3l7qOFZtyJAhpj+HhoYiPDwcLVu2xNatW+Hg4CBhMgKs7Cqe5s2bQ6FQ3DWKOj8/HyqVSqJU1ufOvvqj/ahSqVBQUFBjfXV1NYqKihrlvp4zZw4SEhJw6NAh+Pr6mh5XqVTQ6XQoLi6usf3v9+W99vWddY2FnZ0d2rZti7CwMCxduhSdO3fG8uXLuQ9r6dSpUygoKEC3bt1gY2MDGxsbJCcnY8WKFbCxsYGXlxf350Nyc3NDu3btcPnyZf5eWgCrKih2dnYICwtDUlKS6TGj0YikpCSo1WoJk1mXgIAAqFSqGvtRq9Xi+PHjpv2oVqtRXFyMU6dOmbY5ePAgjEYjwsPD6z2zVIQQmDNnDrZv346DBw8iICCgxvqwsDDY2trW2JeZmZnIysqqsS/PnTtXo/AlJibCxcUFwcHB9fNBLJDRaERVVRX3YS1FRkbi3LlzSE9PNy3du3fHxIkTTX/m/nw4paWluHLlCry9vfl7aQmkHqVbW5s3bxZKpVKsW7dOXLx4UcyaNUu4ubnVGEVNt0f4nzlzRpw5c0YAEB988IE4c+aMuH79uhDi9mXGbm5u4ttvvxVnz54VI0aMuOdlxl27dhXHjx8XR44cEYGBgY3uMuPZs2cLV1dXcfjw4RqXIpaXl5u2eeaZZ4S/v784ePCgSEtLE2q1WqjVatP6O5ciDho0SKSnp4u9e/cKDw+PRnUp4ssvvyySk5PF1atXxdmzZ8XLL78sZDKZ2L9/vxCC+/BR/e9VPEJwfz6ol156SRw+fFhcvXpV/PDDDyIqKko0b95cFBQUCCG4H6VmdQVFCCE++ugj4e/vL+zs7ETPnj3FsWPHpI5kcQ4dOiQA3LVMnTpVCHH7UuPXXntNeHl5CaVSKSIjI0VmZmaN17h586YYP368cHZ2Fi4uLmLatGmipKREgk8jnXvtQwBi7dq1pm0qKirEs88+K5o2bSocHR3FqFGjRG5ubo3XuXbtmhgyZIhwcHAQzZs3Fy+99JLQ6/X1/GmkM336dNGyZUthZ2cnPDw8RGRkpKmcCMF9+Kh+X1C4Px/M2LFjhbe3t7CzsxMtWrQQY8eOFZcvXzat536UlkwIIaQ5dkNERER0b1Y1BoWIiIgaBxYUIiIisjgsKERERGRxWFCIiIjI4rCgEBERkcVhQSEiIiKLw4JCREREFocFhYiIiCwOCwoRERFZHBYUIiIisjgsKERERGRxWFCIiIjI4vw/UvbbneiD5YAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}